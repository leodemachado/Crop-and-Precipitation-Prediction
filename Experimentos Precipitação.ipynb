{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "259a303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3a76641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_num(string=None):\n",
    "    if re.findall('feat', string):\n",
    "        num = int(re.findall('\\d+', string)[0])\n",
    "        return num\n",
    "    else:\n",
    "        return int(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb86985",
   "metadata": {},
   "source": [
    "### Loading City data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c544a001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_58</th>\n",
       "      <th>feat_59</th>\n",
       "      <th>feat_60</th>\n",
       "      <th>feat_61</th>\n",
       "      <th>feat_62</th>\n",
       "      <th>feat_63</th>\n",
       "      <th>feat_64</th>\n",
       "      <th>feat_65</th>\n",
       "      <th>feat_66</th>\n",
       "      <th>PRECIPITACAO TOTAL MENSAL(mm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173.0</td>\n",
       "      <td>233.9</td>\n",
       "      <td>214.7</td>\n",
       "      <td>248.1</td>\n",
       "      <td>230.2</td>\n",
       "      <td>235.3</td>\n",
       "      <td>983.429032</td>\n",
       "      <td>983.982143</td>\n",
       "      <td>982.932258</td>\n",
       "      <td>983.381111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.66</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>233.9</td>\n",
       "      <td>214.7</td>\n",
       "      <td>248.1</td>\n",
       "      <td>230.2</td>\n",
       "      <td>235.3</td>\n",
       "      <td>248.2</td>\n",
       "      <td>983.982143</td>\n",
       "      <td>982.932258</td>\n",
       "      <td>983.381111</td>\n",
       "      <td>984.904301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.85</td>\n",
       "      <td>16.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214.7</td>\n",
       "      <td>248.1</td>\n",
       "      <td>230.2</td>\n",
       "      <td>235.3</td>\n",
       "      <td>248.2</td>\n",
       "      <td>281.2</td>\n",
       "      <td>982.932258</td>\n",
       "      <td>983.381111</td>\n",
       "      <td>984.904301</td>\n",
       "      <td>986.508889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>248.1</td>\n",
       "      <td>230.2</td>\n",
       "      <td>235.3</td>\n",
       "      <td>248.2</td>\n",
       "      <td>281.2</td>\n",
       "      <td>299.1</td>\n",
       "      <td>983.381111</td>\n",
       "      <td>984.904301</td>\n",
       "      <td>986.508889</td>\n",
       "      <td>986.517204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230.2</td>\n",
       "      <td>235.3</td>\n",
       "      <td>248.2</td>\n",
       "      <td>281.2</td>\n",
       "      <td>299.1</td>\n",
       "      <td>313.5</td>\n",
       "      <td>984.904301</td>\n",
       "      <td>986.508889</td>\n",
       "      <td>986.517204</td>\n",
       "      <td>986.092473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_1  feat_2  feat_3  feat_4  feat_5  feat_6      feat_7      feat_8  \\\n",
       "0   173.0   233.9   214.7   248.1   230.2   235.3  983.429032  983.982143   \n",
       "1   233.9   214.7   248.1   230.2   235.3   248.2  983.982143  982.932258   \n",
       "2   214.7   248.1   230.2   235.3   248.2   281.2  982.932258  983.381111   \n",
       "3   248.1   230.2   235.3   248.2   281.2   299.1  983.381111  984.904301   \n",
       "4   230.2   235.3   248.2   281.2   299.1   313.5  984.904301  986.508889   \n",
       "\n",
       "       feat_9     feat_10  ...  feat_58  feat_59  feat_60  feat_61  feat_62  \\\n",
       "0  982.932258  983.381111  ...     0.59     0.59     0.66     0.72     0.74   \n",
       "1  983.381111  984.904301  ...     0.59     0.66     0.56     0.74     0.69   \n",
       "2  984.904301  986.508889  ...     0.66     0.56     0.67     0.69     0.54   \n",
       "3  986.508889  986.517204  ...     0.56     0.67     0.51     0.54     0.54   \n",
       "4  986.517204  986.092473  ...     0.67     0.51     0.49     0.54     0.66   \n",
       "\n",
       "   feat_63  feat_64  feat_65  feat_66  PRECIPITACAO TOTAL MENSAL(mm)  \n",
       "0     0.69     0.54     0.54     0.66                            3.0  \n",
       "1     0.54     0.54     0.66     0.85                           16.6  \n",
       "2     0.54     0.66     0.85     0.59                            0.0  \n",
       "3     0.66     0.85     0.59     0.41                            0.0  \n",
       "4     0.85     0.59     0.41     0.21                            0.3  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cidade = pd.read_csv(\"Experimentos_precipitação/quixeramobim_lags_6_steps_0.csv\")\n",
    "#del fortaleza['Unnamed: 0']\n",
    "cidade.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7ccb8",
   "metadata": {},
   "source": [
    "### Slicing the data set to remove specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09dbf842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5', 'feat_6', 'feat_7',\n",
       "       'feat_8', 'feat_9', 'feat_10', 'feat_11', 'feat_12', 'feat_13',\n",
       "       'feat_14', 'feat_15', 'feat_16', 'feat_17', 'feat_18', 'feat_19',\n",
       "       'feat_20', 'feat_21', 'feat_22', 'feat_23', 'feat_24',\n",
       "       'PRECIPITACAO TOTAL MENSAL(mm)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced = cidade[[f for f in cidade.columns if get_num(f) < 25]]\n",
    "df_reduced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6daad6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cidade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1398654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.regression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db5f6962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_62b96_row42_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_62b96_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_62b96_row0_col0\" class=\"data row0 col0\" >session_id</td>\n",
       "      <td id=\"T_62b96_row0_col1\" class=\"data row0 col1\" >2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_62b96_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_62b96_row1_col1\" class=\"data row1 col1\" >PRECIPITACAO TOTAL MENSAL(mm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_62b96_row2_col0\" class=\"data row2 col0\" >Original Data</td>\n",
       "      <td id=\"T_62b96_row2_col1\" class=\"data row2 col1\" >(189, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_62b96_row3_col0\" class=\"data row3 col0\" >Missing Values</td>\n",
       "      <td id=\"T_62b96_row3_col1\" class=\"data row3 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_62b96_row4_col0\" class=\"data row4 col0\" >Numeric Features</td>\n",
       "      <td id=\"T_62b96_row4_col1\" class=\"data row4 col1\" >24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_62b96_row5_col0\" class=\"data row5 col0\" >Categorical Features</td>\n",
       "      <td id=\"T_62b96_row5_col1\" class=\"data row5 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_62b96_row6_col0\" class=\"data row6 col0\" >Ordinal Features</td>\n",
       "      <td id=\"T_62b96_row6_col1\" class=\"data row6 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_62b96_row7_col0\" class=\"data row7 col0\" >High Cardinality Features</td>\n",
       "      <td id=\"T_62b96_row7_col1\" class=\"data row7 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_62b96_row8_col0\" class=\"data row8 col0\" >High Cardinality Method</td>\n",
       "      <td id=\"T_62b96_row8_col1\" class=\"data row8 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_62b96_row9_col0\" class=\"data row9 col0\" >Transformed Train Set</td>\n",
       "      <td id=\"T_62b96_row9_col1\" class=\"data row9 col1\" >(132, 24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_62b96_row10_col0\" class=\"data row10 col0\" >Transformed Test Set</td>\n",
       "      <td id=\"T_62b96_row10_col1\" class=\"data row10 col1\" >(57, 24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_62b96_row11_col0\" class=\"data row11 col0\" >Shuffle Train-Test</td>\n",
       "      <td id=\"T_62b96_row11_col1\" class=\"data row11 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_62b96_row12_col0\" class=\"data row12 col0\" >Stratify Train-Test</td>\n",
       "      <td id=\"T_62b96_row12_col1\" class=\"data row12 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_62b96_row13_col0\" class=\"data row13 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_62b96_row13_col1\" class=\"data row13 col1\" >KFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_62b96_row14_col0\" class=\"data row14 col0\" >Fold Number</td>\n",
       "      <td id=\"T_62b96_row14_col1\" class=\"data row14 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_62b96_row15_col0\" class=\"data row15 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_62b96_row15_col1\" class=\"data row15 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_62b96_row16_col0\" class=\"data row16 col0\" >Use GPU</td>\n",
       "      <td id=\"T_62b96_row16_col1\" class=\"data row16 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_62b96_row17_col0\" class=\"data row17 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_62b96_row17_col1\" class=\"data row17 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_62b96_row18_col0\" class=\"data row18 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_62b96_row18_col1\" class=\"data row18 col1\" >reg-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_62b96_row19_col0\" class=\"data row19 col0\" >USI</td>\n",
       "      <td id=\"T_62b96_row19_col1\" class=\"data row19 col1\" >2078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_62b96_row20_col0\" class=\"data row20 col0\" >Imputation Type</td>\n",
       "      <td id=\"T_62b96_row20_col1\" class=\"data row20 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_62b96_row21_col0\" class=\"data row21 col0\" >Iterative Imputation Iteration</td>\n",
       "      <td id=\"T_62b96_row21_col1\" class=\"data row21 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_62b96_row22_col0\" class=\"data row22 col0\" >Numeric Imputer</td>\n",
       "      <td id=\"T_62b96_row22_col1\" class=\"data row22 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_62b96_row23_col0\" class=\"data row23 col0\" >Iterative Imputation Numeric Model</td>\n",
       "      <td id=\"T_62b96_row23_col1\" class=\"data row23 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_62b96_row24_col0\" class=\"data row24 col0\" >Categorical Imputer</td>\n",
       "      <td id=\"T_62b96_row24_col1\" class=\"data row24 col1\" >constant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_62b96_row25_col0\" class=\"data row25 col0\" >Iterative Imputation Categorical Model</td>\n",
       "      <td id=\"T_62b96_row25_col1\" class=\"data row25 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_62b96_row26_col0\" class=\"data row26 col0\" >Unknown Categoricals Handling</td>\n",
       "      <td id=\"T_62b96_row26_col1\" class=\"data row26 col1\" >least_frequent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_62b96_row27_col0\" class=\"data row27 col0\" >Normalize</td>\n",
       "      <td id=\"T_62b96_row27_col1\" class=\"data row27 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_62b96_row28_col0\" class=\"data row28 col0\" >Normalize Method</td>\n",
       "      <td id=\"T_62b96_row28_col1\" class=\"data row28 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_62b96_row29_col0\" class=\"data row29 col0\" >Transformation</td>\n",
       "      <td id=\"T_62b96_row29_col1\" class=\"data row29 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_62b96_row30_col0\" class=\"data row30 col0\" >Transformation Method</td>\n",
       "      <td id=\"T_62b96_row30_col1\" class=\"data row30 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_62b96_row31_col0\" class=\"data row31 col0\" >PCA</td>\n",
       "      <td id=\"T_62b96_row31_col1\" class=\"data row31 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "      <td id=\"T_62b96_row32_col0\" class=\"data row32 col0\" >PCA Method</td>\n",
       "      <td id=\"T_62b96_row32_col1\" class=\"data row32 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "      <td id=\"T_62b96_row33_col0\" class=\"data row33 col0\" >PCA Components</td>\n",
       "      <td id=\"T_62b96_row33_col1\" class=\"data row33 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "      <td id=\"T_62b96_row34_col0\" class=\"data row34 col0\" >Ignore Low Variance</td>\n",
       "      <td id=\"T_62b96_row34_col1\" class=\"data row34 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "      <td id=\"T_62b96_row35_col0\" class=\"data row35 col0\" >Combine Rare Levels</td>\n",
       "      <td id=\"T_62b96_row35_col1\" class=\"data row35 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "      <td id=\"T_62b96_row36_col0\" class=\"data row36 col0\" >Rare Level Threshold</td>\n",
       "      <td id=\"T_62b96_row36_col1\" class=\"data row36 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "      <td id=\"T_62b96_row37_col0\" class=\"data row37 col0\" >Numeric Binning</td>\n",
       "      <td id=\"T_62b96_row37_col1\" class=\"data row37 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "      <td id=\"T_62b96_row38_col0\" class=\"data row38 col0\" >Remove Outliers</td>\n",
       "      <td id=\"T_62b96_row38_col1\" class=\"data row38 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
       "      <td id=\"T_62b96_row39_col0\" class=\"data row39 col0\" >Outliers Threshold</td>\n",
       "      <td id=\"T_62b96_row39_col1\" class=\"data row39 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
       "      <td id=\"T_62b96_row40_col0\" class=\"data row40 col0\" >Remove Multicollinearity</td>\n",
       "      <td id=\"T_62b96_row40_col1\" class=\"data row40 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
       "      <td id=\"T_62b96_row41_col0\" class=\"data row41 col0\" >Multicollinearity Threshold</td>\n",
       "      <td id=\"T_62b96_row41_col1\" class=\"data row41 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
       "      <td id=\"T_62b96_row42_col0\" class=\"data row42 col0\" >Remove Perfect Collinearity</td>\n",
       "      <td id=\"T_62b96_row42_col1\" class=\"data row42 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
       "      <td id=\"T_62b96_row43_col0\" class=\"data row43 col0\" >Clustering</td>\n",
       "      <td id=\"T_62b96_row43_col1\" class=\"data row43 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
       "      <td id=\"T_62b96_row44_col0\" class=\"data row44 col0\" >Clustering Iteration</td>\n",
       "      <td id=\"T_62b96_row44_col1\" class=\"data row44 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
       "      <td id=\"T_62b96_row45_col0\" class=\"data row45 col0\" >Polynomial Features</td>\n",
       "      <td id=\"T_62b96_row45_col1\" class=\"data row45 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
       "      <td id=\"T_62b96_row46_col0\" class=\"data row46 col0\" >Polynomial Degree</td>\n",
       "      <td id=\"T_62b96_row46_col1\" class=\"data row46 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
       "      <td id=\"T_62b96_row47_col0\" class=\"data row47 col0\" >Trignometry Features</td>\n",
       "      <td id=\"T_62b96_row47_col1\" class=\"data row47 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
       "      <td id=\"T_62b96_row48_col0\" class=\"data row48 col0\" >Polynomial Threshold</td>\n",
       "      <td id=\"T_62b96_row48_col1\" class=\"data row48 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
       "      <td id=\"T_62b96_row49_col0\" class=\"data row49 col0\" >Group Features</td>\n",
       "      <td id=\"T_62b96_row49_col1\" class=\"data row49 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row50\" class=\"row_heading level0 row50\" >50</th>\n",
       "      <td id=\"T_62b96_row50_col0\" class=\"data row50 col0\" >Feature Selection</td>\n",
       "      <td id=\"T_62b96_row50_col1\" class=\"data row50 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row51\" class=\"row_heading level0 row51\" >51</th>\n",
       "      <td id=\"T_62b96_row51_col0\" class=\"data row51 col0\" >Feature Selection Method</td>\n",
       "      <td id=\"T_62b96_row51_col1\" class=\"data row51 col1\" >classic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row52\" class=\"row_heading level0 row52\" >52</th>\n",
       "      <td id=\"T_62b96_row52_col0\" class=\"data row52 col0\" >Features Selection Threshold</td>\n",
       "      <td id=\"T_62b96_row52_col1\" class=\"data row52 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row53\" class=\"row_heading level0 row53\" >53</th>\n",
       "      <td id=\"T_62b96_row53_col0\" class=\"data row53 col0\" >Feature Interaction</td>\n",
       "      <td id=\"T_62b96_row53_col1\" class=\"data row53 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row54\" class=\"row_heading level0 row54\" >54</th>\n",
       "      <td id=\"T_62b96_row54_col0\" class=\"data row54 col0\" >Feature Ratio</td>\n",
       "      <td id=\"T_62b96_row54_col1\" class=\"data row54 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row55\" class=\"row_heading level0 row55\" >55</th>\n",
       "      <td id=\"T_62b96_row55_col0\" class=\"data row55 col0\" >Interaction Threshold</td>\n",
       "      <td id=\"T_62b96_row55_col1\" class=\"data row55 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row56\" class=\"row_heading level0 row56\" >56</th>\n",
       "      <td id=\"T_62b96_row56_col0\" class=\"data row56 col0\" >Transform Target</td>\n",
       "      <td id=\"T_62b96_row56_col1\" class=\"data row56 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62b96_level0_row57\" class=\"row_heading level0 row57\" >57</th>\n",
       "      <td id=\"T_62b96_row57_col0\" class=\"data row57 col0\" >Transform Target Method</td>\n",
       "      <td id=\"T_62b96_row57_col1\" class=\"data row57 col1\" >box-cox</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f54cc202ca0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "([<pandas.io.formats.style.Styler at 0x7f54cc202ca0>],\n",
       " [],\n",
       " False,\n",
       "          feat_1      feat_2      feat_3      feat_4      feat_5      feat_6  \\\n",
       " 0    173.000000  233.899994  214.699997  248.100006  230.199997  235.300003   \n",
       " 1    233.899994  214.699997  248.100006  230.199997  235.300003  248.199997   \n",
       " 2    214.699997  248.100006  230.199997  235.300003  248.199997  281.200012   \n",
       " 3    248.100006  230.199997  235.300003  248.199997  281.200012  299.100006   \n",
       " 4    230.199997  235.300003  248.199997  281.200012  299.100006  313.500000   \n",
       " ..          ...         ...         ...         ...         ...         ...   \n",
       " 184  238.100006  217.199997  232.500000  267.200012  257.600006  294.000000   \n",
       " 185  217.199997  232.500000  267.200012  257.600006  294.000000  276.000000   \n",
       " 186  232.500000  267.200012  257.600006  294.000000  276.000000  266.700012   \n",
       " 187  267.200012  257.600006  294.000000  276.000000  266.700012  192.699997   \n",
       " 188  257.600006  294.000000  276.000000  266.700012  192.699997  175.899994   \n",
       " \n",
       "          feat_7      feat_8      feat_9     feat_10  ...       feat_15  \\\n",
       " 0    983.429016  983.982117  982.932251  983.381104  ...     29.241289   \n",
       " 1    983.982117  982.932251  983.381104  984.904297  ...  29368.000000   \n",
       " 2    982.932251  983.381104  984.904297  986.508911  ...     28.810968   \n",
       " 3    983.381104  984.904297  986.508911  986.517212  ...     28.422667   \n",
       " 4    984.904297  986.508911  986.517212  986.092468  ...     28.205807   \n",
       " ..          ...         ...         ...         ...  ...           ...   \n",
       " 184  986.309082  988.285583  988.389221  989.376343  ...     26.923870   \n",
       " 185  988.285583  988.389221  989.376343  987.379761  ...     27.468388   \n",
       " 186  988.389221  989.376343  987.379761  986.482178  ...     28.192667   \n",
       " 187  989.376343  987.379761  986.482178  984.714417  ...     28.202759   \n",
       " 188  987.379761  986.482178  984.714417  986.230164  ...  28864.000000   \n",
       " \n",
       "           feat_16       feat_17       feat_18    feat_19    feat_20  \\\n",
       " 0    29368.000000     28.810968     28.422667  66.814514  58.294643   \n",
       " 1       28.810968     28.422667     28.205807  58.294643  62.991936   \n",
       " 2       28.422667     28.205807     28.108387  62.991936  62.408333   \n",
       " 3       28.205807     28.108387     28.597334  62.408333  58.935486   \n",
       " 4       28.108387     28.597334     28.916775  58.935486  49.691666   \n",
       " ..            ...           ...           ...        ...        ...   \n",
       " 184     27.468388     28.192667     28.202759  78.500000  69.449997   \n",
       " 185     28.192667     28.202759  28864.000000  69.449997  66.379028   \n",
       " 186     28.202759  28864.000000     29.396667  66.379028  57.233871   \n",
       " 187  28864.000000     29.396667     28.731333  57.233871  59.008331   \n",
       " 188     29.396667     28.731333     27.215172  59.008331  66.370689   \n",
       " \n",
       "           feat_21       feat_22       feat_23       feat_24  \n",
       " 0       62.991936     62.408333     58.935486     49.691666  \n",
       " 1       62.408333     58.935486     49.691666     49.967743  \n",
       " 2       58.935486     49.691666     49.967743     50.209679  \n",
       " 3       49.691666     49.967743     50.209679  47075.000000  \n",
       " 4       49.967743     50.209679  47075.000000     46.290321  \n",
       " ..            ...           ...           ...           ...  \n",
       " 184     66.379028     57.233871     59.008331     66.370689  \n",
       " 185     57.233871     59.008331     66.370689  61025.000000  \n",
       " 186     59.008331     66.370689  61025.000000     66.033333  \n",
       " 187     66.370689  61025.000000     66.033333     67.183334  \n",
       " 188  61025.000000     66.033333     67.183334     76.051727  \n",
       " \n",
       " [189 rows x 24 columns],\n",
       " -1,\n",
       "      feat_1  feat_2  feat_3  feat_4  feat_5  feat_6      feat_7      feat_8  \\\n",
       " 0     173.0   233.9   214.7   248.1   230.2   235.3  983.429032  983.982143   \n",
       " 1     233.9   214.7   248.1   230.2   235.3   248.2  983.982143  982.932258   \n",
       " 2     214.7   248.1   230.2   235.3   248.2   281.2  982.932258  983.381111   \n",
       " 3     248.1   230.2   235.3   248.2   281.2   299.1  983.381111  984.904301   \n",
       " 4     230.2   235.3   248.2   281.2   299.1   313.5  984.904301  986.508889   \n",
       " ..      ...     ...     ...     ...     ...     ...         ...         ...   \n",
       " 184   238.1   217.2   232.5   267.2   257.6   294.0  986.309091  988.285556   \n",
       " 185   217.2   232.5   267.2   257.6   294.0   276.0  988.285556  988.389247   \n",
       " 186   232.5   267.2   257.6   294.0   276.0   266.7  988.389247  989.376344   \n",
       " 187   267.2   257.6   294.0   276.0   266.7   192.7  989.376344  987.379775   \n",
       " 188   257.6   294.0   276.0   266.7   192.7   175.9  987.379775  986.482192   \n",
       " \n",
       "          feat_9     feat_10  ...       feat_16       feat_17       feat_18  \\\n",
       " 0    982.932258  983.381111  ...  29368.000000     28.810968     28.422667   \n",
       " 1    983.381111  984.904301  ...     28.810968     28.422667     28.205806   \n",
       " 2    984.904301  986.508889  ...     28.422667     28.205806     28.108387   \n",
       " 3    986.508889  986.517204  ...     28.205806     28.108387     28.597333   \n",
       " 4    986.517204  986.092473  ...     28.108387     28.597333     28.916774   \n",
       " ..          ...         ...  ...           ...           ...           ...   \n",
       " 184  988.389247  989.376344  ...     27.468387     28.192667     28.202759   \n",
       " 185  989.376344  987.379775  ...     28.192667     28.202759  28864.000000   \n",
       " 186  987.379775  986.482192  ...     28.202759  28864.000000     29.396667   \n",
       " 187  986.482192  984.714444  ...  28864.000000     29.396667     28.731333   \n",
       " 188  984.714444  986.230159  ...     29.396667     28.731333     27.215172   \n",
       " \n",
       "        feat_19    feat_20       feat_21       feat_22       feat_23  \\\n",
       " 0    66.814516  58.294643     62.991935     62.408333     58.935484   \n",
       " 1    58.294643  62.991935     62.408333     58.935484     49.691667   \n",
       " 2    62.991935  62.408333     58.935484     49.691667     49.967742   \n",
       " 3    62.408333  58.935484     49.691667     49.967742     50.209677   \n",
       " 4    58.935484  49.691667     49.967742     50.209677  47075.000000   \n",
       " ..         ...        ...           ...           ...           ...   \n",
       " 184  78.500000  69.450000     66.379032     57.233871     59.008333   \n",
       " 185  69.450000  66.379032     57.233871     59.008333     66.370690   \n",
       " 186  66.379032  57.233871     59.008333     66.370690  61025.000000   \n",
       " 187  57.233871  59.008333     66.370690  61025.000000     66.033333   \n",
       " 188  59.008333  66.370690  61025.000000     66.033333     67.183333   \n",
       " \n",
       "           feat_24  PRECIPITACAO TOTAL MENSAL(mm)  \n",
       " 0       49.691667                            3.0  \n",
       " 1       49.967742                           16.6  \n",
       " 2       50.209677                            0.0  \n",
       " 3    47075.000000                            0.0  \n",
       " 4       46.290323                            0.3  \n",
       " ..            ...                            ...  \n",
       " 184     66.370690                            0.0  \n",
       " 185  61025.000000                            6.8  \n",
       " 186     66.033333                           77.8  \n",
       " 187     67.183333                          128.0  \n",
       " 188     76.051724                          186.4  \n",
       " \n",
       " [189 rows x 25 columns],\n",
       " 5,\n",
       " 159     12.000000\n",
       " 155      4.000000\n",
       " 127     63.299999\n",
       " 4        0.300000\n",
       " 10     112.599998\n",
       "           ...    \n",
       " 109      0.000000\n",
       " 128     66.199997\n",
       " 57     175.600006\n",
       " 85       0.000000\n",
       " 116    173.100006\n",
       " Name: PRECIPITACAO TOTAL MENSAL(mm), Length: 132, dtype: float32,\n",
       " {'mae': <pycaret.containers.metrics.regression.MAEMetricContainer at 0x7f54cc202730>,\n",
       "  'mse': <pycaret.containers.metrics.regression.MSEMetricContainer at 0x7f54cc202f70>,\n",
       "  'rmse': <pycaret.containers.metrics.regression.RMSEMetricContainer at 0x7f54cc2024f0>,\n",
       "  'r2': <pycaret.containers.metrics.regression.R2MetricContainer at 0x7f54cc202040>,\n",
       "  'rmsle': <pycaret.containers.metrics.regression.RMSLEMetricContainer at 0x7f54cc202880>,\n",
       "  'mape': <pycaret.containers.metrics.regression.MAPEMetricContainer at 0x7f54cc202370>},\n",
       " 0        3.000000\n",
       " 80     139.600006\n",
       " 168      0.000000\n",
       " 141     93.400002\n",
       " 169      0.000000\n",
       " 112     86.900002\n",
       " 77      83.599998\n",
       " 158     23.600000\n",
       " 176     97.500000\n",
       " 66     175.800003\n",
       " 24     122.900002\n",
       " 51     166.699997\n",
       " 182      0.000000\n",
       " 81     132.399994\n",
       " 135      0.000000\n",
       " 56     135.399994\n",
       " 149      6.200000\n",
       " 52      58.200001\n",
       " 64      11.300000\n",
       " 133      0.000000\n",
       " 183      0.000000\n",
       " 170      0.000000\n",
       " 74       0.000000\n",
       " 153    130.899994\n",
       " 164     50.500000\n",
       " 136      0.600000\n",
       " 8      169.600006\n",
       " 42      27.799999\n",
       " 28      82.500000\n",
       " 13       0.000000\n",
       " 79     250.399994\n",
       " 113    137.699997\n",
       " 83      49.000000\n",
       " 22      91.000000\n",
       " 75       0.000000\n",
       " 92     320.700012\n",
       " 163    160.699997\n",
       " 58      40.000000\n",
       " 117    120.699997\n",
       " 14       1.700000\n",
       " 68      96.500000\n",
       " 86       0.000000\n",
       " 36     197.600006\n",
       " 172      6.200000\n",
       " 134      0.300000\n",
       " 129     21.000000\n",
       " 31       0.000000\n",
       " 43       0.000000\n",
       " 131      0.500000\n",
       " 59       9.700000\n",
       " 115    156.600006\n",
       " 23      95.099998\n",
       " 156      0.000000\n",
       " 96      54.099998\n",
       " 2        0.000000\n",
       " 35     305.399994\n",
       " 34      27.400000\n",
       " Name: PRECIPITACAO TOTAL MENSAL(mm), dtype: float32,\n",
       " '2078',\n",
       " {'lr': <pycaret.containers.models.regression.LinearRegressionContainer at 0x7f567198d8b0>,\n",
       "  'lasso': <pycaret.containers.models.regression.LassoRegressionContainer at 0x7f567198d3a0>,\n",
       "  'ridge': <pycaret.containers.models.regression.RidgeRegressionContainer at 0x7f567198dd60>,\n",
       "  'en': <pycaret.containers.models.regression.ElasticNetContainer at 0x7f567198d220>,\n",
       "  'lar': <pycaret.containers.models.regression.LarsContainer at 0x7f567198d4f0>,\n",
       "  'llar': <pycaret.containers.models.regression.LassoLarsContainer at 0x7f5651e1c970>,\n",
       "  'omp': <pycaret.containers.models.regression.OrthogonalMatchingPursuitContainer at 0x7f567198de20>,\n",
       "  'br': <pycaret.containers.models.regression.BayesianRidgeContainer at 0x7f567198d5e0>,\n",
       "  'ard': <pycaret.containers.models.regression.AutomaticRelevanceDeterminationContainer at 0x7f54cc212f70>,\n",
       "  'par': <pycaret.containers.models.regression.PassiveAggressiveRegressorContainer at 0x7f54cc238f70>,\n",
       "  'ransac': <pycaret.containers.models.regression.RANSACRegressorContainer at 0x7f54cc2381f0>,\n",
       "  'tr': <pycaret.containers.models.regression.TheilSenRegressorContainer at 0x7f54cc238af0>,\n",
       "  'huber': <pycaret.containers.models.regression.HuberRegressorContainer at 0x7f54cc238eb0>,\n",
       "  'kr': <pycaret.containers.models.regression.KernelRidgeContainer at 0x7f5671b60340>,\n",
       "  'svm': <pycaret.containers.models.regression.SVRContainer at 0x7f5671b604c0>,\n",
       "  'knn': <pycaret.containers.models.regression.KNeighborsRegressorContainer at 0x7f5671b60460>,\n",
       "  'dt': <pycaret.containers.models.regression.DecisionTreeRegressorContainer at 0x7f5671b60ac0>,\n",
       "  'rf': <pycaret.containers.models.regression.RandomForestRegressorContainer at 0x7f5671b60bb0>,\n",
       "  'et': <pycaret.containers.models.regression.ExtraTreesRegressorContainer at 0x7f54cc65e580>,\n",
       "  'ada': <pycaret.containers.models.regression.AdaBoostRegressorContainer at 0x7f54cc370e50>,\n",
       "  'gbr': <pycaret.containers.models.regression.GradientBoostingRegressorContainer at 0x7f54cc370250>,\n",
       "  'mlp': <pycaret.containers.models.regression.MLPRegressorContainer at 0x7f54cc370820>,\n",
       "  'xgboost': <pycaret.containers.models.regression.XGBRegressorContainer at 0x7f563fc972e0>,\n",
       "  'lightgbm': <pycaret.containers.models.regression.LGBMRegressorContainer at 0x7f563fc97250>,\n",
       "  'catboost': <pycaret.containers.models.regression.CatBoostRegressorContainer at 0x7f563fc97880>,\n",
       "  'Bagging': <pycaret.containers.models.regression.BaggingRegressorContainer at 0x7f563fc97bb0>,\n",
       "  'Stacking': <pycaret.containers.models.regression.StackingRegressorContainer at 0x7f563fc97d90>,\n",
       "  'Voting': <pycaret.containers.models.regression.VotingRegressorContainer at 0x7f563fc976d0>},\n",
       " Pipeline(memory=None, steps=[('empty_step', 'passthrough')], verbose=False),\n",
       " 'lightgbm',\n",
       " False,\n",
       " None,\n",
       " 'lightgbm',\n",
       " 0        3.000000\n",
       " 1       16.600000\n",
       " 2        0.000000\n",
       " 3        0.000000\n",
       " 4        0.300000\n",
       "           ...    \n",
       " 184      0.000000\n",
       " 185      6.800000\n",
       " 186     77.800003\n",
       " 187    128.000000\n",
       " 188    186.399994\n",
       " Name: PRECIPITACAO TOTAL MENSAL(mm), Length: 189, dtype: float32,\n",
       " 2021,\n",
       " 'box-cox',\n",
       " False,\n",
       " 10,\n",
       " Pipeline(memory=None,\n",
       "          steps=[('dtypes',\n",
       "                  DataTypes_Auto_infer(categorical_features=[],\n",
       "                                       display_types=True, features_todrop=[],\n",
       "                                       id_columns=[], ml_usecase='regression',\n",
       "                                       numerical_features=[],\n",
       "                                       target='PRECIPITACAO TOTAL MENSAL(mm)',\n",
       "                                       time_features=[])),\n",
       "                 ('imputer',\n",
       "                  Simple_Imputer(categorical_strategy='not_available',\n",
       "                                 fill_value_categorical=None,\n",
       "                                 fill_value_numerical=N...\n",
       "                 ('binn', 'passthrough'), ('rem_outliers', 'passthrough'),\n",
       "                 ('cluster_all', 'passthrough'),\n",
       "                 ('dummy', Dummify(target='PRECIPITACAO TOTAL MENSAL(mm)')),\n",
       "                 ('fix_perfect',\n",
       "                  Remove_100(target='PRECIPITACAO TOTAL MENSAL(mm)')),\n",
       "                 ('clean_names', Clean_Colum_Names()),\n",
       "                 ('feature_select', 'passthrough'), ('fix_multi', 'passthrough'),\n",
       "                 ('dfs', 'passthrough'), ('pca', 'passthrough')],\n",
       "          verbose=False),\n",
       " 'PRECIPITACAO TOTAL MENSAL(mm)',\n",
       "          feat_1      feat_2      feat_3      feat_4      feat_5      feat_6  \\\n",
       " 0    173.000000  233.899994  214.699997  248.100006  230.199997  235.300003   \n",
       " 80   304.700012  288.700012  233.399994  211.100006  227.000000  167.199997   \n",
       " 168  196.199997  191.500000  235.399994  261.500000  246.199997  214.899994   \n",
       " 141  281.100006  261.200012  227.899994  237.600006  227.100006  202.600006   \n",
       " 169  191.500000  235.399994  261.500000  246.199997  214.899994  311.200012   \n",
       " 112  262.899994  283.100006  294.600006  305.200012  251.300003  299.500000   \n",
       " 77   284.500000  275.899994  297.000000  304.700012  288.700012  233.399994   \n",
       " 158  203.399994  228.500000  252.500000  281.299988  305.000000  263.100006   \n",
       " 176  268.899994  277.799988  254.500000  223.199997  227.100006  170.699997   \n",
       " 66   289.200012  283.399994  304.600006  270.700012  259.000000  266.799988   \n",
       " 24   273.600006  284.600006  312.299988  288.100006  257.600006  239.399994   \n",
       " 51   285.799988  260.100006  214.300003  201.100006  194.500000  247.500000   \n",
       " 182  188.100006  157.199997  238.100006  217.199997  232.500000  267.200012   \n",
       " 81   288.700012  233.399994  211.100006  227.000000  167.199997  179.300003   \n",
       " 135  274.899994  237.899994  279.600006  307.799988  304.200012  308.399994   \n",
       " 56   317.700012  299.000000  271.100006  289.799988  170.600006  232.899994   \n",
       " 149  259.799988  279.700012  295.200012  306.100006  240.600006  268.500000   \n",
       " 52   260.100006  214.300003  201.100006  194.500000  247.500000  222.399994   \n",
       " 64   215.300003  264.200012  289.200012  283.399994  304.600006  270.700012   \n",
       " 133  252.699997  231.600006  274.899994  237.899994  279.600006  307.799988   \n",
       " 183  157.199997  238.100006  217.199997  232.500000  267.200012  257.600006   \n",
       " 170  235.399994  261.500000  246.199997  214.899994  311.200012  282.899994   \n",
       " 74   193.000000  241.600006  235.100006  284.500000  275.899994  297.000000   \n",
       " 153  240.600006  268.500000  231.199997  188.000000  220.800003  203.399994   \n",
       " 164  291.799988  255.899994  266.200012  276.600006  196.199997  191.500000   \n",
       " 136  237.899994  279.600006  307.799988  304.200012  308.399994  281.100006   \n",
       " 8    299.100006  313.500000  222.699997  271.100006  244.699997  210.100006   \n",
       " 42   175.800003  237.000000  235.899994  248.600006  188.199997  251.699997   \n",
       " 28   257.600006  239.399994  161.800003  218.600006  229.300003  219.699997   \n",
       " 13   210.100006  216.699997  219.100006  211.800003  273.600006  257.799988   \n",
       " 79   297.000000  304.700012  288.700012  233.399994  211.100006  227.000000   \n",
       " 113  283.100006  294.600006  305.200012  251.300003  299.500000  200.000000   \n",
       " 83   211.100006  227.000000  167.199997  179.300003  218.800003  214.399994   \n",
       " 22   242.399994  212.100006  217.399994  200.699997  208.699997  215.300003   \n",
       " 75   241.600006  235.100006  284.500000  275.899994  297.000000  304.700012   \n",
       " 92   318.299988  301.600006  281.299988  224.100006  171.199997  186.600006   \n",
       " 163  263.100006  291.799988  255.899994  266.200012  276.600006  196.199997   \n",
       " 58   271.100006  289.799988  170.600006  232.899994  197.800003  223.199997   \n",
       " 117  299.500000  200.000000  192.600006  186.600006  223.300003  195.399994   \n",
       " 14   216.699997  219.100006  211.800003  273.600006  257.799988  309.899994   \n",
       " 68   304.600006  270.700012  259.000000  266.799988  147.399994  226.500000   \n",
       " 86   179.300003  218.800003  214.399994  229.399994  269.200012  292.600006   \n",
       " 36   289.399994  296.399994  308.200012  245.300003  279.299988  155.399994   \n",
       " 172  246.199997  214.899994  311.200012  282.899994  304.000000  305.500000   \n",
       " 134  231.600006  274.899994  237.899994  279.600006  307.799988  304.200012   \n",
       " 129  278.200012  290.299988  239.899994  191.699997  252.699997  231.600006   \n",
       " 31   218.600006  229.300003  219.699997  246.800003  274.100006  289.399994   \n",
       " 43   237.000000  235.899994  248.600006  188.199997  251.699997  295.799988   \n",
       " 131  239.899994  191.699997  252.699997  231.600006  274.899994  237.899994   \n",
       " 59   289.799988  170.600006  232.899994  197.800003  223.199997  215.300003   \n",
       " 115  305.200012  251.300003  299.500000  200.000000  192.600006  186.600006   \n",
       " 23   212.100006  217.399994  200.699997  208.699997  215.300003  239.000000   \n",
       " 156  188.000000  220.800003  203.399994  228.500000  252.500000  281.299988   \n",
       " 96   171.199997  186.600006  158.000000  136.399994  222.199997  223.300003   \n",
       " 2    214.699997  248.100006  230.199997  235.300003  248.199997  281.200012   \n",
       " 35   274.100006  289.399994  296.399994  308.200012  245.300003  279.299988   \n",
       " 34   246.800003  274.100006  289.399994  296.399994  308.200012  245.300003   \n",
       " \n",
       "             feat_7         feat_8         feat_9        feat_10  ...  \\\n",
       " 0       983.429016     983.982117     982.932251     983.381104  ...   \n",
       " 80      982.778503     984.843323     984.815063     984.636536  ...   \n",
       " 168     983.191650     984.515076     986.176697     985.813965  ...   \n",
       " 141     985.798889     985.546265     986.197449     985.703552  ...   \n",
       " 169     984.515076     986.176697     985.813965     987.101135  ...   \n",
       " 112     987.298889     987.880615     988.211853     986.159973  ...   \n",
       " 77      987.847290     985.295532     984.695557     982.778503  ...   \n",
       " 158     985.133362     985.018250     985.792053     986.761047  ...   \n",
       " 176     986.837219     986.267761     985.094421     985.926880  ...   \n",
       " 66      986.936584     986.961121     985.962341     985.435547  ...   \n",
       " 24      987.873108     987.220459     985.533325     985.377808  ...   \n",
       " 51      983.119995     982.908630     982.767761     982.094055  ...   \n",
       " 182     986.419373     986.251099     986.309082     988.285583  ...   \n",
       " 81      984.843323     984.815063     984.636536     985.549438  ...   \n",
       " 135     986.390320  988345.000000     987.896179     988.849487  ...   \n",
       " 56      984.437622     983.957764     982.363464     982.843018  ...   \n",
       " 149     988.823059     987.632263     987.513306     986.635498  ...   \n",
       " 52      982.908630     982.767761     982.094055     983.184937  ...   \n",
       " 64      987.520020     987.897827     986.936584     986.961121  ...   \n",
       " 133     985.932251     985.622192     986.390320  988345.000000  ...   \n",
       " 183     986.251099     986.309082     988.285583     988.389221  ...   \n",
       " 170     986.176697     985.813965     987.101135     986.692322  ...   \n",
       " 74      986.359985     986.845154     988.130005     987.847290  ...   \n",
       " 153     985.917786     985.867737     985.734619     985.065491  ...   \n",
       " 164     983.433350     984.386658     983.941956     982.916687  ...   \n",
       " 136  988345.000000     987.896179     988.849487     988.266663  ...   \n",
       " 8       985.803345     985.255920     983.476685     983.303223  ...   \n",
       " 42      984.237915     983.850525     984.023315     984.916138  ...   \n",
       " 28      985.572815     985.864502     985.696411     985.684937  ...   \n",
       " 13      983.265503     983.250549     984.659973     984.960205  ...   \n",
       " 79      984.695557     982.778503     984.843323     984.815063  ...   \n",
       " 113     987.880615     988.211853     986.159973     984.888184  ...   \n",
       " 83      984.636536     985.549438     984.678467     984.477783  ...   \n",
       " 22      982.675537     985.116150     984.996765     985.894226  ...   \n",
       " 75      986.845154     988.130005     987.847290     985.295532  ...   \n",
       " 92      986.582825     984.491089     984.931213     986.753784  ...   \n",
       " 163     983.803345     983.433350     984.386658     983.941956  ...   \n",
       " 58      982.363464     982.843018     983.209534     984.183899  ...   \n",
       " 117     984.513306     984.758057     985.546265     987.187744  ...   \n",
       " 14      983.250549     984.659973     984.960205     985.270020  ...   \n",
       " 68      985.962341     985.435547     987.069885     986.507507  ...   \n",
       " 86      984.477783     985.784973     987.751099     988.684937  ...   \n",
       " 36      987.888184     987.281128     986.075256     985.591125  ...   \n",
       " 172     987.101135     986.692322     984.421509     985.332214  ...   \n",
       " 134     985.622192     986.390320  988345.000000     987.896179  ...   \n",
       " 129     987.074463     986.974182     987.705139     986.696533  ...   \n",
       " 31      985.684937     986.080017     986.874207     987.374451  ...   \n",
       " 43      983.850525     984.023315     984.916138     986.649414  ...   \n",
       " 131     987.705139     986.696533     985.932251     985.622192  ...   \n",
       " 59      982.843018     983.209534     984.183899     984.468872  ...   \n",
       " 115     986.159973     984.888184     984.513306     984.758057  ...   \n",
       " 23      985.116150     984.996765     985.894226     985.081726  ...   \n",
       " 156     985.065491     985.559143     985.133362     985.018250  ...   \n",
       " 96      987.038086     986.553772     987.556641     987.497864  ...   \n",
       " 2       982.932251     983.381104     984.904297     986.508911  ...   \n",
       " 35      988.201050     987.888184     987.281128     986.075256  ...   \n",
       " 34      987.374451     988.201050     987.888184     987.281128  ...   \n",
       " \n",
       "           feat_15       feat_16       feat_17       feat_18       feat_19  \\\n",
       " 0       29.241289  29368.000000     28.810968     28.422667     66.814514   \n",
       " 80      27.925161     27.649033     27.497240     25.387741     50.879032   \n",
       " 168  26956.000000     28.172258     27.600000     27.176773     72.053574   \n",
       " 141     29.025806     28.846430     28.592258     27.454666     60.541668   \n",
       " 169     28.172258     27.600000     27.176773     27.693548     78.266129   \n",
       " 112     27.599356  27918.000000     28.188387     28.819332     63.410713   \n",
       " 77   27918.000000     28.101934  28334.000000     27.925161     54.379032   \n",
       " 158     26.687332     27.129032     27.963226     28.110001     85.283333   \n",
       " 176     28.894667     28.429676     27.428387     26.850000     58.348213   \n",
       " 66      28.249031  28064.000000     28.496128     28.879999     51.620968   \n",
       " 24      28.446451     28.555332     29.118065     28.323872     56.400002   \n",
       " 51      28.709032     29.077143     28.006453     27.440666     58.750000   \n",
       " 182     26.818066     27.231724     26.923870     27.468388     83.709679   \n",
       " 81      27.649033     27.497240     25.387741     24.843332     51.700001   \n",
       " 135     27.477419     27.750969  27728.000000     27.992258     65.161293   \n",
       " 56      28.788387     28.598064     27.842142     26.598064     50.153225   \n",
       " 149  28074.000000     28.224516     27.641333     27.969032     78.193550   \n",
       " 52      29.077143     28.006453     27.440666     25.941935     57.822582   \n",
       " 64      27.029676     27.850000     28.249031  28064.000000     66.358330   \n",
       " 133     28.040646  27542.000000     27.477419     27.750969     69.040321   \n",
       " 183     27.231724     26.923870     27.468388     28.192667     85.608330   \n",
       " 170     27.600000     27.176773     27.693548     27.863333     79.158333   \n",
       " 74      25.701332     26.725161  27006.000000  27918.000000     68.000000   \n",
       " 153     28.220646     27.439285     26.760645  26764.000000     66.433334   \n",
       " 164     28.460646     28.538710     27.911428     26.878065     63.241936   \n",
       " 136     27.750969  27728.000000     27.992258     28.305332     69.766670   \n",
       " 8       28.761333     28.910322     28.294193     28.184286  47075.000000   \n",
       " 42      25.985332     25.525806     24.355333     24.703871     85.922417   \n",
       " 28      27.392857     26.447098     26.790001     26.566452     56.629032   \n",
       " 13      26.914667     25.572903     26.350668     26.858709     59.357143   \n",
       " 79   28334.000000     27.925161     27.649033     27.497240  49375.000000   \n",
       " 113  27918.000000     28.188387     28.819332     27.288387     54.822582   \n",
       " 83      25.387741     24.843332     24.878710     24.330000     56.451614   \n",
       " 22      27.021935     27.017931     26.917419     25.990000     52.391666   \n",
       " 75      26.725161  27006.000000  27918.000000     28.101934     63.056454   \n",
       " 92      28.868387     28.102581     26.738571     25.836775     48.846775   \n",
       " 163     27.949333     28.460646     28.538710     27.911428     65.341667   \n",
       " 58      27.842142     26.598064     26.139999     25.232258     54.153225   \n",
       " 117     25.906452     25.684286     25.494194  25484.000000  57225.000000   \n",
       " 14      25.572903     26.350668     26.858709     27.625160     71.137100   \n",
       " 68      28.496128     28.879999     27.020000     26.101290     50.846775   \n",
       " 86      24.330000     25.288387     26.438709     28.159332     65.966667   \n",
       " 36      28.494194     28.949333     28.549032     26.810324     51.209679   \n",
       " 172     27.693548     27.863333     28.410969     28.889334     68.250000   \n",
       " 134  27542.000000     27.477419     27.750969  27728.000000     66.491669   \n",
       " 129     27.874840     27.048277     27.322580  27916.000000     70.266670   \n",
       " 31      26.566452     25.770000     26.746452     27.701935     77.758064   \n",
       " 43      25.525806     24.355333     24.703871     26.578711     84.024193   \n",
       " 131     27.322580  27916.000000     28.040646  27542.000000     71.822578   \n",
       " 59      26.598064     26.139999     25.232258     24.765333     55.629032   \n",
       " 115     28.819332     27.288387     25.906452     25.684286     51.458332   \n",
       " 23      27.017931     26.917419     25.990000     25.692259     67.032257   \n",
       " 156  26764.000000     26.070324     26.687332     27.129032     76.642860   \n",
       " 96      25.256666     24.909033     24.670000     24.801291     69.223213   \n",
       " 2       28.810968     28.422667     28.205807     28.108387     62.991936   \n",
       " 35      28.424667     28.494194     28.949333     28.549032     52.056454   \n",
       " 34      27.701935     28.424667     28.494194     28.949333     72.608330   \n",
       " \n",
       "           feat_20       feat_21       feat_22       feat_23       feat_24  \n",
       " 0       58.294643     62.991936     62.408333     58.935486     49.691666  \n",
       " 80      51.700001     55.290321     56.451614     58.603447     68.604836  \n",
       " 168     78.266129     79.158333     69.056450     68.250000  72275.000000  \n",
       " 141     60.443546     65.129028     60.080357     62.709679     74.508331  \n",
       " 169     79.158333     69.056450     68.250000  72275.000000     61.911289  \n",
       " 112     54.822582     51.185486     51.458332     62.854839  57225.000000  \n",
       " 77      51.049999  49375.000000     50.879032     51.700001     55.290321  \n",
       " 158     85.056450     80.891670     69.189651     61.290321     65.341667  \n",
       " 176     61.854839     61.283333     63.653225     70.298386     80.767860  \n",
       " 66      50.058334     50.846775  64275.000000     64.282257     63.225807  \n",
       " 24      56.183334     52.959679     58.908333     56.629032     61.475807  \n",
       " 51      57.822582     59.903225     62.258930     67.532257  71025.000000  \n",
       " 182     85.608330     78.500000     69.449997     66.379028     57.233871  \n",
       " 81      55.290321     56.451614     58.603447     68.604836     65.966667  \n",
       " 135     69.766670     67.991936     63.725807     64.949997     65.040321  \n",
       " 56      52.033333     54.153225     55.629032     62.464287     66.145164  \n",
       " 149     65.790321     65.258331     65.080643     66.433334     65.854836  \n",
       " 52      59.903225     62.258930     67.532257  71025.000000     78.750000  \n",
       " 64      60.225807     51.620968     50.058334     50.846775  64275.000000  \n",
       " 133     66.491669     65.161293     69.766670     67.991936     63.725807  \n",
       " 183     78.500000     69.449997     66.379028     57.233871     59.008331  \n",
       " 170     69.056450     68.250000  72275.000000     61.911289     65.683334  \n",
       " 74      63.056454     64.958336     54.379032     51.049999  49375.000000  \n",
       " 153     65.854836     75.346771     76.642860     83.072578     85.283333  \n",
       " 164     66.150002     65.750000     66.209679     72.053574     78.266129  \n",
       " 136     67.991936     63.725807     64.949997     65.040321     60.541668  \n",
       " 8       46.290321  49425.000000     49.814514     57.677418     59.357143  \n",
       " 42      84.024193     77.083336     73.596771  82625.000000     76.298386  \n",
       " 28      61.475807     71.035713     77.758064     78.391670     77.717743  \n",
       " 13      71.137100     67.800003     75.750000     58.791668     49.911289  \n",
       " 79      50.879032     51.700001     55.290321     56.451614     58.603447  \n",
       " 113     51.185486     51.458332     62.854839  57225.000000     73.120972  \n",
       " 83      58.603447     68.604836     65.966667     66.056450  63775.000000  \n",
       " 22      67.032257     72.943550     74.491379     77.806450     77.983330  \n",
       " 75      64.958336     54.379032     51.049999  49375.000000     50.879032  \n",
       " 92   53175.000000     54.637096     59.556454     69.223213     75.500000  \n",
       " 163     63.241936     66.150002     65.750000     66.209679     72.053574  \n",
       " 58      55.629032     62.464287     66.145164     70.583336     70.258064  \n",
       " 117     73.120972     78.854836     85.482140     81.838707  82925.000000  \n",
       " 14      67.800003     75.750000     58.791668     49.911289     46.298386  \n",
       " 68   64275.000000     64.282257     63.225807     74.357140     74.169357  \n",
       " 86      66.056450  63775.000000     59.612904     51.879032     46.549999  \n",
       " 36      51.612068     55.443546     58.666668     72.362900     84.072578  \n",
       " 172  72275.000000     61.911289     65.683334     65.927422  64575.000000  \n",
       " 134     65.161293     69.766670     67.991936     63.725807     64.949997  \n",
       " 129     67.919357     71.822578     73.870689     69.040321     66.491669  \n",
       " 31      78.391670     77.717743     72.608330     52.056454     51.209679  \n",
       " 43      77.083336     73.596771  82625.000000     76.298386     63.774193  \n",
       " 131     73.870689     69.040321     66.491669     65.161293     69.766670  \n",
       " 59      62.464287     66.145164     70.583336     70.258064     66.358330  \n",
       " 115     62.854839  57225.000000     73.120972     78.854836     85.482140  \n",
       " 23      72.943550     74.491379     77.806450     77.983330     78.540321  \n",
       " 156     83.072578     85.283333     85.056450     80.891670     69.189651  \n",
       " 96      75.500000     74.916664     77.612900     76.641670     73.620972  \n",
       " 2       62.408333     58.935486     49.691666     49.967743     50.209679  \n",
       " 35      51.209679     51.612068     55.443546     58.666668     72.362900  \n",
       " 34      52.056454     51.209679     51.612068     55.443546     58.666668  \n",
       " \n",
       " [57 rows x 24 columns],\n",
       " [('Setup Config',\n",
       "                                  Description                          Value\n",
       "   0                               session_id                           2021\n",
       "   1                                   Target  PRECIPITACAO TOTAL MENSAL(mm)\n",
       "   2                            Original Data                      (189, 25)\n",
       "   3                           Missing Values                          False\n",
       "   4                         Numeric Features                             24\n",
       "   5                     Categorical Features                              0\n",
       "   6                         Ordinal Features                          False\n",
       "   7                High Cardinality Features                          False\n",
       "   8                  High Cardinality Method                           None\n",
       "   9                    Transformed Train Set                      (132, 24)\n",
       "   10                    Transformed Test Set                       (57, 24)\n",
       "   11                      Shuffle Train-Test                           True\n",
       "   12                     Stratify Train-Test                          False\n",
       "   13                          Fold Generator                          KFold\n",
       "   14                             Fold Number                             10\n",
       "   15                                CPU Jobs                             -1\n",
       "   16                                 Use GPU                          False\n",
       "   17                          Log Experiment                          False\n",
       "   18                         Experiment Name               reg-default-name\n",
       "   19                                     USI                           2078\n",
       "   20                         Imputation Type                         simple\n",
       "   21          Iterative Imputation Iteration                           None\n",
       "   22                         Numeric Imputer                           mean\n",
       "   23      Iterative Imputation Numeric Model                           None\n",
       "   24                     Categorical Imputer                       constant\n",
       "   25  Iterative Imputation Categorical Model                           None\n",
       "   26           Unknown Categoricals Handling                 least_frequent\n",
       "   27                               Normalize                          False\n",
       "   28                        Normalize Method                           None\n",
       "   29                          Transformation                          False\n",
       "   30                   Transformation Method                           None\n",
       "   31                                     PCA                          False\n",
       "   32                              PCA Method                           None\n",
       "   33                          PCA Components                           None\n",
       "   34                     Ignore Low Variance                          False\n",
       "   35                     Combine Rare Levels                          False\n",
       "   36                    Rare Level Threshold                           None\n",
       "   37                         Numeric Binning                          False\n",
       "   38                         Remove Outliers                          False\n",
       "   39                      Outliers Threshold                           None\n",
       "   40                Remove Multicollinearity                          False\n",
       "   41             Multicollinearity Threshold                           None\n",
       "   42             Remove Perfect Collinearity                           True\n",
       "   43                              Clustering                          False\n",
       "   44                    Clustering Iteration                           None\n",
       "   45                     Polynomial Features                          False\n",
       "   46                       Polynomial Degree                           None\n",
       "   47                    Trignometry Features                          False\n",
       "   48                    Polynomial Threshold                           None\n",
       "   49                          Group Features                          False\n",
       "   50                       Feature Selection                          False\n",
       "   51                Feature Selection Method                        classic\n",
       "   52            Features Selection Threshold                           None\n",
       "   53                     Feature Interaction                          False\n",
       "   54                           Feature Ratio                          False\n",
       "   55                   Interaction Threshold                           None\n",
       "   56                        Transform Target                          False\n",
       "   57                 Transform Target Method                        box-cox),\n",
       "  ('X_training Set',\n",
       "            feat_1      feat_2      feat_3      feat_4      feat_5      feat_6  \\\n",
       "   159  228.500000  252.500000  281.299988  305.000000  263.100006  291.799988   \n",
       "   155  231.199997  188.000000  220.800003  203.399994  228.500000  252.500000   \n",
       "   127  295.500000  253.000000  278.200012  290.299988  239.899994  191.699997   \n",
       "   4    230.199997  235.300003  248.199997  281.200012  299.100006  313.500000   \n",
       "   10   222.699997  271.100006  244.699997  210.100006  216.699997  219.100006   \n",
       "   ..          ...         ...         ...         ...         ...         ...   \n",
       "   109  235.399994  211.100006  251.800003  262.899994  283.100006  294.600006   \n",
       "   128  253.000000  278.200012  290.299988  239.899994  191.699997  252.699997   \n",
       "   57   299.000000  271.100006  289.799988  170.600006  232.899994  197.800003   \n",
       "   85   167.199997  179.300003  218.800003  214.399994  229.399994  269.200012   \n",
       "   116  251.300003  299.500000  200.000000  192.600006  186.600006  223.300003   \n",
       "   \n",
       "            feat_7      feat_8      feat_9     feat_10  ...       feat_15  \\\n",
       "   159  985.018250  985.792053  986.761047  985.390320  ...     27.129032   \n",
       "   155  985.734619  985.065491  985.559143  985.133362  ...     26.760645   \n",
       "   127  989.298889  987.918274  987.074463  986.974182  ...  27724.000000   \n",
       "   4    984.904297  986.508911  986.517212  986.092468  ...     28.205807   \n",
       "   10   983.476685  983.303223  983.061279  983.265503  ...     28.294193   \n",
       "   ..          ...         ...         ...         ...  ...           ...   \n",
       "   109  983.975281  984.288879  984.557007  987.298889  ...     26.948387   \n",
       "   128  987.918274  987.074463  986.974182  987.705139  ...     27.969032   \n",
       "   57   983.957764  982.363464  982.843018  983.209534  ...     28.598064   \n",
       "   85   984.678467  984.477783  985.784973  987.751099  ...     24.878710   \n",
       "   116  984.888184  984.513306  984.758057  985.546265  ...     27.288387   \n",
       "   \n",
       "             feat_16    feat_17    feat_18       feat_19       feat_20  \\\n",
       "   159     27.963226  28.110001  28.342581     85.056450     80.891670   \n",
       "   155  26764.000000  26.070324  26.687332     75.346771     76.642860   \n",
       "   127     27.969032  27.874840  27.048277     64.308334     75.830643   \n",
       "   4       28.108387  28.597334  28.916775     58.935486     49.691666   \n",
       "   10      28.184286  26.738710  26.914667  49425.000000     49.814514   \n",
       "   ..            ...        ...        ...           ...           ...   \n",
       "   109     26.114668  27.329678  27.599356     47.112904     55.541668   \n",
       "   128     27.874840  27.048277  27.322580     75.830643     70.266670   \n",
       "   57      27.842142  26.598064  26.139999     52.033333     54.153225   \n",
       "   85      24.330000  25.288387  26.438709     68.604836     65.966667   \n",
       "   116     25.906452  25.684286  25.494194     62.854839  57225.000000   \n",
       "   \n",
       "          feat_21       feat_22       feat_23    feat_24  \n",
       "   159  69.189651     61.290321     65.341667  63.241936  \n",
       "   155  83.072578     85.283333     85.056450  80.891670  \n",
       "   127  70.266670     67.919357     71.822578  73.870689  \n",
       "   4    49.967743     50.209679  47075.000000  46.290321  \n",
       "   10   57.677418     59.357143     71.137100  67.800003  \n",
       "   ..         ...           ...           ...        ...  \n",
       "   109  54.274193     63.410713     54.822582  51.185486  \n",
       "   128  67.919357     71.822578     73.870689  69.040321  \n",
       "   57   55.629032     62.464287     66.145164  70.583336  \n",
       "   85   66.056450  63775.000000     59.612904  51.879032  \n",
       "   116  73.120972     78.854836     85.482140  81.838707  \n",
       "   \n",
       "   [132 rows x 24 columns]),\n",
       "  ('y_training Set',\n",
       "   159     12.000000\n",
       "   155      4.000000\n",
       "   127     63.299999\n",
       "   4        0.300000\n",
       "   10     112.599998\n",
       "             ...    \n",
       "   109      0.000000\n",
       "   128     66.199997\n",
       "   57     175.600006\n",
       "   85       0.000000\n",
       "   116    173.100006\n",
       "   Name: PRECIPITACAO TOTAL MENSAL(mm), Length: 132, dtype: float32),\n",
       "  ('X_test Set',\n",
       "            feat_1      feat_2      feat_3      feat_4      feat_5      feat_6  \\\n",
       "   0    173.000000  233.899994  214.699997  248.100006  230.199997  235.300003   \n",
       "   80   304.700012  288.700012  233.399994  211.100006  227.000000  167.199997   \n",
       "   168  196.199997  191.500000  235.399994  261.500000  246.199997  214.899994   \n",
       "   141  281.100006  261.200012  227.899994  237.600006  227.100006  202.600006   \n",
       "   169  191.500000  235.399994  261.500000  246.199997  214.899994  311.200012   \n",
       "   112  262.899994  283.100006  294.600006  305.200012  251.300003  299.500000   \n",
       "   77   284.500000  275.899994  297.000000  304.700012  288.700012  233.399994   \n",
       "   158  203.399994  228.500000  252.500000  281.299988  305.000000  263.100006   \n",
       "   176  268.899994  277.799988  254.500000  223.199997  227.100006  170.699997   \n",
       "   66   289.200012  283.399994  304.600006  270.700012  259.000000  266.799988   \n",
       "   24   273.600006  284.600006  312.299988  288.100006  257.600006  239.399994   \n",
       "   51   285.799988  260.100006  214.300003  201.100006  194.500000  247.500000   \n",
       "   182  188.100006  157.199997  238.100006  217.199997  232.500000  267.200012   \n",
       "   81   288.700012  233.399994  211.100006  227.000000  167.199997  179.300003   \n",
       "   135  274.899994  237.899994  279.600006  307.799988  304.200012  308.399994   \n",
       "   56   317.700012  299.000000  271.100006  289.799988  170.600006  232.899994   \n",
       "   149  259.799988  279.700012  295.200012  306.100006  240.600006  268.500000   \n",
       "   52   260.100006  214.300003  201.100006  194.500000  247.500000  222.399994   \n",
       "   64   215.300003  264.200012  289.200012  283.399994  304.600006  270.700012   \n",
       "   133  252.699997  231.600006  274.899994  237.899994  279.600006  307.799988   \n",
       "   183  157.199997  238.100006  217.199997  232.500000  267.200012  257.600006   \n",
       "   170  235.399994  261.500000  246.199997  214.899994  311.200012  282.899994   \n",
       "   74   193.000000  241.600006  235.100006  284.500000  275.899994  297.000000   \n",
       "   153  240.600006  268.500000  231.199997  188.000000  220.800003  203.399994   \n",
       "   164  291.799988  255.899994  266.200012  276.600006  196.199997  191.500000   \n",
       "   136  237.899994  279.600006  307.799988  304.200012  308.399994  281.100006   \n",
       "   8    299.100006  313.500000  222.699997  271.100006  244.699997  210.100006   \n",
       "   42   175.800003  237.000000  235.899994  248.600006  188.199997  251.699997   \n",
       "   28   257.600006  239.399994  161.800003  218.600006  229.300003  219.699997   \n",
       "   13   210.100006  216.699997  219.100006  211.800003  273.600006  257.799988   \n",
       "   79   297.000000  304.700012  288.700012  233.399994  211.100006  227.000000   \n",
       "   113  283.100006  294.600006  305.200012  251.300003  299.500000  200.000000   \n",
       "   83   211.100006  227.000000  167.199997  179.300003  218.800003  214.399994   \n",
       "   22   242.399994  212.100006  217.399994  200.699997  208.699997  215.300003   \n",
       "   75   241.600006  235.100006  284.500000  275.899994  297.000000  304.700012   \n",
       "   92   318.299988  301.600006  281.299988  224.100006  171.199997  186.600006   \n",
       "   163  263.100006  291.799988  255.899994  266.200012  276.600006  196.199997   \n",
       "   58   271.100006  289.799988  170.600006  232.899994  197.800003  223.199997   \n",
       "   117  299.500000  200.000000  192.600006  186.600006  223.300003  195.399994   \n",
       "   14   216.699997  219.100006  211.800003  273.600006  257.799988  309.899994   \n",
       "   68   304.600006  270.700012  259.000000  266.799988  147.399994  226.500000   \n",
       "   86   179.300003  218.800003  214.399994  229.399994  269.200012  292.600006   \n",
       "   36   289.399994  296.399994  308.200012  245.300003  279.299988  155.399994   \n",
       "   172  246.199997  214.899994  311.200012  282.899994  304.000000  305.500000   \n",
       "   134  231.600006  274.899994  237.899994  279.600006  307.799988  304.200012   \n",
       "   129  278.200012  290.299988  239.899994  191.699997  252.699997  231.600006   \n",
       "   31   218.600006  229.300003  219.699997  246.800003  274.100006  289.399994   \n",
       "   43   237.000000  235.899994  248.600006  188.199997  251.699997  295.799988   \n",
       "   131  239.899994  191.699997  252.699997  231.600006  274.899994  237.899994   \n",
       "   59   289.799988  170.600006  232.899994  197.800003  223.199997  215.300003   \n",
       "   115  305.200012  251.300003  299.500000  200.000000  192.600006  186.600006   \n",
       "   23   212.100006  217.399994  200.699997  208.699997  215.300003  239.000000   \n",
       "   156  188.000000  220.800003  203.399994  228.500000  252.500000  281.299988   \n",
       "   96   171.199997  186.600006  158.000000  136.399994  222.199997  223.300003   \n",
       "   2    214.699997  248.100006  230.199997  235.300003  248.199997  281.200012   \n",
       "   35   274.100006  289.399994  296.399994  308.200012  245.300003  279.299988   \n",
       "   34   246.800003  274.100006  289.399994  296.399994  308.200012  245.300003   \n",
       "   \n",
       "               feat_7         feat_8         feat_9        feat_10  ...  \\\n",
       "   0       983.429016     983.982117     982.932251     983.381104  ...   \n",
       "   80      982.778503     984.843323     984.815063     984.636536  ...   \n",
       "   168     983.191650     984.515076     986.176697     985.813965  ...   \n",
       "   141     985.798889     985.546265     986.197449     985.703552  ...   \n",
       "   169     984.515076     986.176697     985.813965     987.101135  ...   \n",
       "   112     987.298889     987.880615     988.211853     986.159973  ...   \n",
       "   77      987.847290     985.295532     984.695557     982.778503  ...   \n",
       "   158     985.133362     985.018250     985.792053     986.761047  ...   \n",
       "   176     986.837219     986.267761     985.094421     985.926880  ...   \n",
       "   66      986.936584     986.961121     985.962341     985.435547  ...   \n",
       "   24      987.873108     987.220459     985.533325     985.377808  ...   \n",
       "   51      983.119995     982.908630     982.767761     982.094055  ...   \n",
       "   182     986.419373     986.251099     986.309082     988.285583  ...   \n",
       "   81      984.843323     984.815063     984.636536     985.549438  ...   \n",
       "   135     986.390320  988345.000000     987.896179     988.849487  ...   \n",
       "   56      984.437622     983.957764     982.363464     982.843018  ...   \n",
       "   149     988.823059     987.632263     987.513306     986.635498  ...   \n",
       "   52      982.908630     982.767761     982.094055     983.184937  ...   \n",
       "   64      987.520020     987.897827     986.936584     986.961121  ...   \n",
       "   133     985.932251     985.622192     986.390320  988345.000000  ...   \n",
       "   183     986.251099     986.309082     988.285583     988.389221  ...   \n",
       "   170     986.176697     985.813965     987.101135     986.692322  ...   \n",
       "   74      986.359985     986.845154     988.130005     987.847290  ...   \n",
       "   153     985.917786     985.867737     985.734619     985.065491  ...   \n",
       "   164     983.433350     984.386658     983.941956     982.916687  ...   \n",
       "   136  988345.000000     987.896179     988.849487     988.266663  ...   \n",
       "   8       985.803345     985.255920     983.476685     983.303223  ...   \n",
       "   42      984.237915     983.850525     984.023315     984.916138  ...   \n",
       "   28      985.572815     985.864502     985.696411     985.684937  ...   \n",
       "   13      983.265503     983.250549     984.659973     984.960205  ...   \n",
       "   79      984.695557     982.778503     984.843323     984.815063  ...   \n",
       "   113     987.880615     988.211853     986.159973     984.888184  ...   \n",
       "   83      984.636536     985.549438     984.678467     984.477783  ...   \n",
       "   22      982.675537     985.116150     984.996765     985.894226  ...   \n",
       "   75      986.845154     988.130005     987.847290     985.295532  ...   \n",
       "   92      986.582825     984.491089     984.931213     986.753784  ...   \n",
       "   163     983.803345     983.433350     984.386658     983.941956  ...   \n",
       "   58      982.363464     982.843018     983.209534     984.183899  ...   \n",
       "   117     984.513306     984.758057     985.546265     987.187744  ...   \n",
       "   14      983.250549     984.659973     984.960205     985.270020  ...   \n",
       "   68      985.962341     985.435547     987.069885     986.507507  ...   \n",
       "   86      984.477783     985.784973     987.751099     988.684937  ...   \n",
       "   36      987.888184     987.281128     986.075256     985.591125  ...   \n",
       "   172     987.101135     986.692322     984.421509     985.332214  ...   \n",
       "   134     985.622192     986.390320  988345.000000     987.896179  ...   \n",
       "   129     987.074463     986.974182     987.705139     986.696533  ...   \n",
       "   31      985.684937     986.080017     986.874207     987.374451  ...   \n",
       "   43      983.850525     984.023315     984.916138     986.649414  ...   \n",
       "   131     987.705139     986.696533     985.932251     985.622192  ...   \n",
       "   59      982.843018     983.209534     984.183899     984.468872  ...   \n",
       "   115     986.159973     984.888184     984.513306     984.758057  ...   \n",
       "   23      985.116150     984.996765     985.894226     985.081726  ...   \n",
       "   156     985.065491     985.559143     985.133362     985.018250  ...   \n",
       "   96      987.038086     986.553772     987.556641     987.497864  ...   \n",
       "   2       982.932251     983.381104     984.904297     986.508911  ...   \n",
       "   35      988.201050     987.888184     987.281128     986.075256  ...   \n",
       "   34      987.374451     988.201050     987.888184     987.281128  ...   \n",
       "   \n",
       "             feat_15       feat_16       feat_17       feat_18       feat_19  \\\n",
       "   0       29.241289  29368.000000     28.810968     28.422667     66.814514   \n",
       "   80      27.925161     27.649033     27.497240     25.387741     50.879032   \n",
       "   168  26956.000000     28.172258     27.600000     27.176773     72.053574   \n",
       "   141     29.025806     28.846430     28.592258     27.454666     60.541668   \n",
       "   169     28.172258     27.600000     27.176773     27.693548     78.266129   \n",
       "   112     27.599356  27918.000000     28.188387     28.819332     63.410713   \n",
       "   77   27918.000000     28.101934  28334.000000     27.925161     54.379032   \n",
       "   158     26.687332     27.129032     27.963226     28.110001     85.283333   \n",
       "   176     28.894667     28.429676     27.428387     26.850000     58.348213   \n",
       "   66      28.249031  28064.000000     28.496128     28.879999     51.620968   \n",
       "   24      28.446451     28.555332     29.118065     28.323872     56.400002   \n",
       "   51      28.709032     29.077143     28.006453     27.440666     58.750000   \n",
       "   182     26.818066     27.231724     26.923870     27.468388     83.709679   \n",
       "   81      27.649033     27.497240     25.387741     24.843332     51.700001   \n",
       "   135     27.477419     27.750969  27728.000000     27.992258     65.161293   \n",
       "   56      28.788387     28.598064     27.842142     26.598064     50.153225   \n",
       "   149  28074.000000     28.224516     27.641333     27.969032     78.193550   \n",
       "   52      29.077143     28.006453     27.440666     25.941935     57.822582   \n",
       "   64      27.029676     27.850000     28.249031  28064.000000     66.358330   \n",
       "   133     28.040646  27542.000000     27.477419     27.750969     69.040321   \n",
       "   183     27.231724     26.923870     27.468388     28.192667     85.608330   \n",
       "   170     27.600000     27.176773     27.693548     27.863333     79.158333   \n",
       "   74      25.701332     26.725161  27006.000000  27918.000000     68.000000   \n",
       "   153     28.220646     27.439285     26.760645  26764.000000     66.433334   \n",
       "   164     28.460646     28.538710     27.911428     26.878065     63.241936   \n",
       "   136     27.750969  27728.000000     27.992258     28.305332     69.766670   \n",
       "   8       28.761333     28.910322     28.294193     28.184286  47075.000000   \n",
       "   42      25.985332     25.525806     24.355333     24.703871     85.922417   \n",
       "   28      27.392857     26.447098     26.790001     26.566452     56.629032   \n",
       "   13      26.914667     25.572903     26.350668     26.858709     59.357143   \n",
       "   79   28334.000000     27.925161     27.649033     27.497240  49375.000000   \n",
       "   113  27918.000000     28.188387     28.819332     27.288387     54.822582   \n",
       "   83      25.387741     24.843332     24.878710     24.330000     56.451614   \n",
       "   22      27.021935     27.017931     26.917419     25.990000     52.391666   \n",
       "   75      26.725161  27006.000000  27918.000000     28.101934     63.056454   \n",
       "   92      28.868387     28.102581     26.738571     25.836775     48.846775   \n",
       "   163     27.949333     28.460646     28.538710     27.911428     65.341667   \n",
       "   58      27.842142     26.598064     26.139999     25.232258     54.153225   \n",
       "   117     25.906452     25.684286     25.494194  25484.000000  57225.000000   \n",
       "   14      25.572903     26.350668     26.858709     27.625160     71.137100   \n",
       "   68      28.496128     28.879999     27.020000     26.101290     50.846775   \n",
       "   86      24.330000     25.288387     26.438709     28.159332     65.966667   \n",
       "   36      28.494194     28.949333     28.549032     26.810324     51.209679   \n",
       "   172     27.693548     27.863333     28.410969     28.889334     68.250000   \n",
       "   134  27542.000000     27.477419     27.750969  27728.000000     66.491669   \n",
       "   129     27.874840     27.048277     27.322580  27916.000000     70.266670   \n",
       "   31      26.566452     25.770000     26.746452     27.701935     77.758064   \n",
       "   43      25.525806     24.355333     24.703871     26.578711     84.024193   \n",
       "   131     27.322580  27916.000000     28.040646  27542.000000     71.822578   \n",
       "   59      26.598064     26.139999     25.232258     24.765333     55.629032   \n",
       "   115     28.819332     27.288387     25.906452     25.684286     51.458332   \n",
       "   23      27.017931     26.917419     25.990000     25.692259     67.032257   \n",
       "   156  26764.000000     26.070324     26.687332     27.129032     76.642860   \n",
       "   96      25.256666     24.909033     24.670000     24.801291     69.223213   \n",
       "   2       28.810968     28.422667     28.205807     28.108387     62.991936   \n",
       "   35      28.424667     28.494194     28.949333     28.549032     52.056454   \n",
       "   34      27.701935     28.424667     28.494194     28.949333     72.608330   \n",
       "   \n",
       "             feat_20       feat_21       feat_22       feat_23       feat_24  \n",
       "   0       58.294643     62.991936     62.408333     58.935486     49.691666  \n",
       "   80      51.700001     55.290321     56.451614     58.603447     68.604836  \n",
       "   168     78.266129     79.158333     69.056450     68.250000  72275.000000  \n",
       "   141     60.443546     65.129028     60.080357     62.709679     74.508331  \n",
       "   169     79.158333     69.056450     68.250000  72275.000000     61.911289  \n",
       "   112     54.822582     51.185486     51.458332     62.854839  57225.000000  \n",
       "   77      51.049999  49375.000000     50.879032     51.700001     55.290321  \n",
       "   158     85.056450     80.891670     69.189651     61.290321     65.341667  \n",
       "   176     61.854839     61.283333     63.653225     70.298386     80.767860  \n",
       "   66      50.058334     50.846775  64275.000000     64.282257     63.225807  \n",
       "   24      56.183334     52.959679     58.908333     56.629032     61.475807  \n",
       "   51      57.822582     59.903225     62.258930     67.532257  71025.000000  \n",
       "   182     85.608330     78.500000     69.449997     66.379028     57.233871  \n",
       "   81      55.290321     56.451614     58.603447     68.604836     65.966667  \n",
       "   135     69.766670     67.991936     63.725807     64.949997     65.040321  \n",
       "   56      52.033333     54.153225     55.629032     62.464287     66.145164  \n",
       "   149     65.790321     65.258331     65.080643     66.433334     65.854836  \n",
       "   52      59.903225     62.258930     67.532257  71025.000000     78.750000  \n",
       "   64      60.225807     51.620968     50.058334     50.846775  64275.000000  \n",
       "   133     66.491669     65.161293     69.766670     67.991936     63.725807  \n",
       "   183     78.500000     69.449997     66.379028     57.233871     59.008331  \n",
       "   170     69.056450     68.250000  72275.000000     61.911289     65.683334  \n",
       "   74      63.056454     64.958336     54.379032     51.049999  49375.000000  \n",
       "   153     65.854836     75.346771     76.642860     83.072578     85.283333  \n",
       "   164     66.150002     65.750000     66.209679     72.053574     78.266129  \n",
       "   136     67.991936     63.725807     64.949997     65.040321     60.541668  \n",
       "   8       46.290321  49425.000000     49.814514     57.677418     59.357143  \n",
       "   42      84.024193     77.083336     73.596771  82625.000000     76.298386  \n",
       "   28      61.475807     71.035713     77.758064     78.391670     77.717743  \n",
       "   13      71.137100     67.800003     75.750000     58.791668     49.911289  \n",
       "   79      50.879032     51.700001     55.290321     56.451614     58.603447  \n",
       "   113     51.185486     51.458332     62.854839  57225.000000     73.120972  \n",
       "   83      58.603447     68.604836     65.966667     66.056450  63775.000000  \n",
       "   22      67.032257     72.943550     74.491379     77.806450     77.983330  \n",
       "   75      64.958336     54.379032     51.049999  49375.000000     50.879032  \n",
       "   92   53175.000000     54.637096     59.556454     69.223213     75.500000  \n",
       "   163     63.241936     66.150002     65.750000     66.209679     72.053574  \n",
       "   58      55.629032     62.464287     66.145164     70.583336     70.258064  \n",
       "   117     73.120972     78.854836     85.482140     81.838707  82925.000000  \n",
       "   14      67.800003     75.750000     58.791668     49.911289     46.298386  \n",
       "   68   64275.000000     64.282257     63.225807     74.357140     74.169357  \n",
       "   86      66.056450  63775.000000     59.612904     51.879032     46.549999  \n",
       "   36      51.612068     55.443546     58.666668     72.362900     84.072578  \n",
       "   172  72275.000000     61.911289     65.683334     65.927422  64575.000000  \n",
       "   134     65.161293     69.766670     67.991936     63.725807     64.949997  \n",
       "   129     67.919357     71.822578     73.870689     69.040321     66.491669  \n",
       "   31      78.391670     77.717743     72.608330     52.056454     51.209679  \n",
       "   43      77.083336     73.596771  82625.000000     76.298386     63.774193  \n",
       "   131     73.870689     69.040321     66.491669     65.161293     69.766670  \n",
       "   59      62.464287     66.145164     70.583336     70.258064     66.358330  \n",
       "   115     62.854839  57225.000000     73.120972     78.854836     85.482140  \n",
       "   23      72.943550     74.491379     77.806450     77.983330     78.540321  \n",
       "   156     83.072578     85.283333     85.056450     80.891670     69.189651  \n",
       "   96      75.500000     74.916664     77.612900     76.641670     73.620972  \n",
       "   2       62.408333     58.935486     49.691666     49.967743     50.209679  \n",
       "   35      51.209679     51.612068     55.443546     58.666668     72.362900  \n",
       "   34      52.056454     51.209679     51.612068     55.443546     58.666668  \n",
       "   \n",
       "   [57 rows x 24 columns]),\n",
       "  ('y_test Set',\n",
       "   0        3.000000\n",
       "   80     139.600006\n",
       "   168      0.000000\n",
       "   141     93.400002\n",
       "   169      0.000000\n",
       "   112     86.900002\n",
       "   77      83.599998\n",
       "   158     23.600000\n",
       "   176     97.500000\n",
       "   66     175.800003\n",
       "   24     122.900002\n",
       "   51     166.699997\n",
       "   182      0.000000\n",
       "   81     132.399994\n",
       "   135      0.000000\n",
       "   56     135.399994\n",
       "   149      6.200000\n",
       "   52      58.200001\n",
       "   64      11.300000\n",
       "   133      0.000000\n",
       "   183      0.000000\n",
       "   170      0.000000\n",
       "   74       0.000000\n",
       "   153    130.899994\n",
       "   164     50.500000\n",
       "   136      0.600000\n",
       "   8      169.600006\n",
       "   42      27.799999\n",
       "   28      82.500000\n",
       "   13       0.000000\n",
       "   79     250.399994\n",
       "   113    137.699997\n",
       "   83      49.000000\n",
       "   22      91.000000\n",
       "   75       0.000000\n",
       "   92     320.700012\n",
       "   163    160.699997\n",
       "   58      40.000000\n",
       "   117    120.699997\n",
       "   14       1.700000\n",
       "   68      96.500000\n",
       "   86       0.000000\n",
       "   36     197.600006\n",
       "   172      6.200000\n",
       "   134      0.300000\n",
       "   129     21.000000\n",
       "   31       0.000000\n",
       "   43       0.000000\n",
       "   131      0.500000\n",
       "   59       9.700000\n",
       "   115    156.600006\n",
       "   23      95.099998\n",
       "   156      0.000000\n",
       "   96      54.099998\n",
       "   2        0.000000\n",
       "   35     305.399994\n",
       "   34      27.400000\n",
       "   Name: PRECIPITACAO TOTAL MENSAL(mm), dtype: float32),\n",
       "  ('Transformation Pipeline',\n",
       "   Pipeline(memory=None,\n",
       "            steps=[('dtypes',\n",
       "                    DataTypes_Auto_infer(categorical_features=[],\n",
       "                                         display_types=True, features_todrop=[],\n",
       "                                         id_columns=[], ml_usecase='regression',\n",
       "                                         numerical_features=[],\n",
       "                                         target='PRECIPITACAO TOTAL MENSAL(mm)',\n",
       "                                         time_features=[])),\n",
       "                   ('imputer',\n",
       "                    Simple_Imputer(categorical_strategy='not_available',\n",
       "                                   fill_value_categorical=None,\n",
       "                                   fill_value_numerical=N...\n",
       "                   ('binn', 'passthrough'), ('rem_outliers', 'passthrough'),\n",
       "                   ('cluster_all', 'passthrough'),\n",
       "                   ('dummy', Dummify(target='PRECIPITACAO TOTAL MENSAL(mm)')),\n",
       "                   ('fix_perfect',\n",
       "                    Remove_100(target='PRECIPITACAO TOTAL MENSAL(mm)')),\n",
       "                   ('clean_names', Clean_Colum_Names()),\n",
       "                   ('feature_select', 'passthrough'), ('fix_multi', 'passthrough'),\n",
       "                   ('dfs', 'passthrough'), ('pca', 'passthrough')],\n",
       "            verbose=False))],\n",
       " [],\n",
       " None,\n",
       " False,\n",
       " {'parameter': 'Hyperparameters',\n",
       "  'residuals': 'Residuals',\n",
       "  'error': 'Prediction Error',\n",
       "  'cooks': 'Cooks Distance',\n",
       "  'rfe': 'Feature Selection',\n",
       "  'learning': 'Learning Curve',\n",
       "  'manifold': 'Manifold Learning',\n",
       "  'vc': 'Validation Curve',\n",
       "  'feature': 'Feature Importance',\n",
       "  'feature_all': 'Feature Importance (All)',\n",
       "  'tree': 'Decision Tree',\n",
       "  'residuals_interactive': 'Interactive Residuals'},\n",
       " KFold(n_splits=10, random_state=None, shuffle=False),\n",
       " <MLUsecase.REGRESSION: 2>,\n",
       "          feat_1      feat_2      feat_3      feat_4      feat_5      feat_6  \\\n",
       " 159  228.500000  252.500000  281.299988  305.000000  263.100006  291.799988   \n",
       " 155  231.199997  188.000000  220.800003  203.399994  228.500000  252.500000   \n",
       " 127  295.500000  253.000000  278.200012  290.299988  239.899994  191.699997   \n",
       " 4    230.199997  235.300003  248.199997  281.200012  299.100006  313.500000   \n",
       " 10   222.699997  271.100006  244.699997  210.100006  216.699997  219.100006   \n",
       " ..          ...         ...         ...         ...         ...         ...   \n",
       " 109  235.399994  211.100006  251.800003  262.899994  283.100006  294.600006   \n",
       " 128  253.000000  278.200012  290.299988  239.899994  191.699997  252.699997   \n",
       " 57   299.000000  271.100006  289.799988  170.600006  232.899994  197.800003   \n",
       " 85   167.199997  179.300003  218.800003  214.399994  229.399994  269.200012   \n",
       " 116  251.300003  299.500000  200.000000  192.600006  186.600006  223.300003   \n",
       " \n",
       "          feat_7      feat_8      feat_9     feat_10  ...       feat_15  \\\n",
       " 159  985.018250  985.792053  986.761047  985.390320  ...     27.129032   \n",
       " 155  985.734619  985.065491  985.559143  985.133362  ...     26.760645   \n",
       " 127  989.298889  987.918274  987.074463  986.974182  ...  27724.000000   \n",
       " 4    984.904297  986.508911  986.517212  986.092468  ...     28.205807   \n",
       " 10   983.476685  983.303223  983.061279  983.265503  ...     28.294193   \n",
       " ..          ...         ...         ...         ...  ...           ...   \n",
       " 109  983.975281  984.288879  984.557007  987.298889  ...     26.948387   \n",
       " 128  987.918274  987.074463  986.974182  987.705139  ...     27.969032   \n",
       " 57   983.957764  982.363464  982.843018  983.209534  ...     28.598064   \n",
       " 85   984.678467  984.477783  985.784973  987.751099  ...     24.878710   \n",
       " 116  984.888184  984.513306  984.758057  985.546265  ...     27.288387   \n",
       " \n",
       "           feat_16    feat_17    feat_18       feat_19       feat_20  \\\n",
       " 159     27.963226  28.110001  28.342581     85.056450     80.891670   \n",
       " 155  26764.000000  26.070324  26.687332     75.346771     76.642860   \n",
       " 127     27.969032  27.874840  27.048277     64.308334     75.830643   \n",
       " 4       28.108387  28.597334  28.916775     58.935486     49.691666   \n",
       " 10      28.184286  26.738710  26.914667  49425.000000     49.814514   \n",
       " ..            ...        ...        ...           ...           ...   \n",
       " 109     26.114668  27.329678  27.599356     47.112904     55.541668   \n",
       " 128     27.874840  27.048277  27.322580     75.830643     70.266670   \n",
       " 57      27.842142  26.598064  26.139999     52.033333     54.153225   \n",
       " 85      24.330000  25.288387  26.438709     68.604836     65.966667   \n",
       " 116     25.906452  25.684286  25.494194     62.854839  57225.000000   \n",
       " \n",
       "        feat_21       feat_22       feat_23    feat_24  \n",
       " 159  69.189651     61.290321     65.341667  63.241936  \n",
       " 155  83.072578     85.283333     85.056450  80.891670  \n",
       " 127  70.266670     67.919357     71.822578  73.870689  \n",
       " 4    49.967743     50.209679  47075.000000  46.290321  \n",
       " 10   57.677418     59.357143     71.137100  67.800003  \n",
       " ..         ...           ...           ...        ...  \n",
       " 109  54.274193     63.410713     54.822582  51.185486  \n",
       " 128  67.919357     71.822578     73.870689  69.040321  \n",
       " 57   55.629032     62.464287     66.145164  70.583336  \n",
       " 85   66.056450  63775.000000     59.612904  51.879032  \n",
       " 116  73.120972     78.854836     85.482140  81.838707  \n",
       " \n",
       " [132 rows x 24 columns],\n",
       " -1,\n",
       " {'USI',\n",
       "  'X',\n",
       "  'X_test',\n",
       "  'X_train',\n",
       "  '_all_metrics',\n",
       "  '_all_models',\n",
       "  '_all_models_internal',\n",
       "  '_available_plots',\n",
       "  '_gpu_n_jobs_param',\n",
       "  '_internal_pipeline',\n",
       "  '_ml_usecase',\n",
       "  'create_model_container',\n",
       "  'data_before_preprocess',\n",
       "  'display_container',\n",
       "  'exp_name_log',\n",
       "  'experiment__',\n",
       "  'fix_imbalance_method_param',\n",
       "  'fix_imbalance_param',\n",
       "  'fold_generator',\n",
       "  'fold_groups_param',\n",
       "  'fold_groups_param_full',\n",
       "  'fold_param',\n",
       "  'fold_shuffle_param',\n",
       "  'gpu_param',\n",
       "  'html_param',\n",
       "  'imputation_classifier',\n",
       "  'imputation_regressor',\n",
       "  'iterative_imputation_iters_param',\n",
       "  'log_plots_param',\n",
       "  'logging_param',\n",
       "  'master_model_container',\n",
       "  'n_jobs_param',\n",
       "  'prep_pipe',\n",
       "  'pycaret_globals',\n",
       "  'seed',\n",
       "  'stratify_param',\n",
       "  'target_param',\n",
       "  'transform_target_method_param',\n",
       "  'transform_target_param',\n",
       "  'y',\n",
       "  'y_test',\n",
       "  'y_train'},\n",
       " 'reg-default-name',\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " {'lr': <pycaret.containers.models.regression.LinearRegressionContainer at 0x7f56719d1b50>,\n",
       "  'lasso': <pycaret.containers.models.regression.LassoRegressionContainer at 0x7f56719d1eb0>,\n",
       "  'ridge': <pycaret.containers.models.regression.RidgeRegressionContainer at 0x7f56719d1cd0>,\n",
       "  'en': <pycaret.containers.models.regression.ElasticNetContainer at 0x7f56719d1af0>,\n",
       "  'lar': <pycaret.containers.models.regression.LarsContainer at 0x7f56719d1df0>,\n",
       "  'llar': <pycaret.containers.models.regression.LassoLarsContainer at 0x7f54cc27dee0>,\n",
       "  'omp': <pycaret.containers.models.regression.OrthogonalMatchingPursuitContainer at 0x7f54cc27d700>,\n",
       "  'br': <pycaret.containers.models.regression.BayesianRidgeContainer at 0x7f54cc27d910>,\n",
       "  'ard': <pycaret.containers.models.regression.AutomaticRelevanceDeterminationContainer at 0x7f567194ccd0>,\n",
       "  'par': <pycaret.containers.models.regression.PassiveAggressiveRegressorContainer at 0x7f563fc80eb0>,\n",
       "  'ransac': <pycaret.containers.models.regression.RANSACRegressorContainer at 0x7f563fc80220>,\n",
       "  'tr': <pycaret.containers.models.regression.TheilSenRegressorContainer at 0x7f563fc807f0>,\n",
       "  'huber': <pycaret.containers.models.regression.HuberRegressorContainer at 0x7f563fc806a0>,\n",
       "  'kr': <pycaret.containers.models.regression.KernelRidgeContainer at 0x7f563fc801f0>,\n",
       "  'svm': <pycaret.containers.models.regression.SVRContainer at 0x7f563fc80d60>,\n",
       "  'knn': <pycaret.containers.models.regression.KNeighborsRegressorContainer at 0x7f563fc802e0>,\n",
       "  'dt': <pycaret.containers.models.regression.DecisionTreeRegressorContainer at 0x7f54cc6565e0>,\n",
       "  'rf': <pycaret.containers.models.regression.RandomForestRegressorContainer at 0x7f563fc7aeb0>,\n",
       "  'et': <pycaret.containers.models.regression.ExtraTreesRegressorContainer at 0x7f563fc7aee0>,\n",
       "  'ada': <pycaret.containers.models.regression.AdaBoostRegressorContainer at 0x7f563fc7a190>,\n",
       "  'gbr': <pycaret.containers.models.regression.GradientBoostingRegressorContainer at 0x7f5671b61ee0>,\n",
       "  'mlp': <pycaret.containers.models.regression.MLPRegressorContainer at 0x7f5671b61e20>,\n",
       "  'xgboost': <pycaret.containers.models.regression.XGBRegressorContainer at 0x7f5671a94850>,\n",
       "  'lightgbm': <pycaret.containers.models.regression.LGBMRegressorContainer at 0x7f5671a94640>,\n",
       "  'catboost': <pycaret.containers.models.regression.CatBoostRegressorContainer at 0x7f5671a94fd0>},\n",
       " False,\n",
       " None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup( df_reduced, session_id=2021, target='PRECIPITACAO TOTAL MENSAL(mm)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "302c9ba0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a23ff_ th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_a23ff_row0_col0, #T_a23ff_row0_col1, #T_a23ff_row0_col2, #T_a23ff_row0_col3, #T_a23ff_row0_col5, #T_a23ff_row0_col6, #T_a23ff_row1_col0, #T_a23ff_row1_col4, #T_a23ff_row1_col5, #T_a23ff_row1_col6, #T_a23ff_row2_col0, #T_a23ff_row2_col1, #T_a23ff_row2_col2, #T_a23ff_row2_col3, #T_a23ff_row2_col4, #T_a23ff_row2_col5, #T_a23ff_row2_col6, #T_a23ff_row3_col0, #T_a23ff_row3_col1, #T_a23ff_row3_col2, #T_a23ff_row3_col3, #T_a23ff_row3_col4, #T_a23ff_row3_col5, #T_a23ff_row3_col6, #T_a23ff_row4_col0, #T_a23ff_row4_col1, #T_a23ff_row4_col2, #T_a23ff_row4_col3, #T_a23ff_row4_col4, #T_a23ff_row4_col5, #T_a23ff_row5_col0, #T_a23ff_row5_col1, #T_a23ff_row5_col2, #T_a23ff_row5_col3, #T_a23ff_row5_col4, #T_a23ff_row5_col5, #T_a23ff_row5_col6, #T_a23ff_row6_col0, #T_a23ff_row6_col1, #T_a23ff_row6_col2, #T_a23ff_row6_col3, #T_a23ff_row6_col4, #T_a23ff_row6_col5, #T_a23ff_row6_col6, #T_a23ff_row7_col0, #T_a23ff_row7_col1, #T_a23ff_row7_col2, #T_a23ff_row7_col3, #T_a23ff_row7_col4, #T_a23ff_row7_col5, #T_a23ff_row7_col6, #T_a23ff_row8_col0, #T_a23ff_row8_col1, #T_a23ff_row8_col2, #T_a23ff_row8_col3, #T_a23ff_row8_col4, #T_a23ff_row8_col5, #T_a23ff_row8_col6, #T_a23ff_row9_col0, #T_a23ff_row9_col1, #T_a23ff_row9_col2, #T_a23ff_row9_col3, #T_a23ff_row9_col4, #T_a23ff_row9_col6, #T_a23ff_row10_col0, #T_a23ff_row10_col1, #T_a23ff_row10_col2, #T_a23ff_row10_col3, #T_a23ff_row10_col4, #T_a23ff_row10_col5, #T_a23ff_row10_col6, #T_a23ff_row11_col0, #T_a23ff_row11_col1, #T_a23ff_row11_col2, #T_a23ff_row11_col3, #T_a23ff_row11_col4, #T_a23ff_row11_col5, #T_a23ff_row11_col6, #T_a23ff_row12_col0, #T_a23ff_row12_col1, #T_a23ff_row12_col2, #T_a23ff_row12_col3, #T_a23ff_row12_col4, #T_a23ff_row12_col5, #T_a23ff_row12_col6, #T_a23ff_row13_col0, #T_a23ff_row13_col1, #T_a23ff_row13_col2, #T_a23ff_row13_col3, #T_a23ff_row13_col4, #T_a23ff_row13_col5, #T_a23ff_row13_col6, #T_a23ff_row14_col0, #T_a23ff_row14_col1, #T_a23ff_row14_col2, #T_a23ff_row14_col3, #T_a23ff_row14_col4, #T_a23ff_row14_col5, #T_a23ff_row14_col6, #T_a23ff_row15_col0, #T_a23ff_row15_col1, #T_a23ff_row15_col2, #T_a23ff_row15_col3, #T_a23ff_row15_col4, #T_a23ff_row15_col5, #T_a23ff_row15_col6, #T_a23ff_row16_col0, #T_a23ff_row16_col1, #T_a23ff_row16_col2, #T_a23ff_row16_col3, #T_a23ff_row16_col4, #T_a23ff_row16_col5, #T_a23ff_row16_col6, #T_a23ff_row17_col0, #T_a23ff_row17_col1, #T_a23ff_row17_col2, #T_a23ff_row17_col3, #T_a23ff_row17_col4, #T_a23ff_row17_col5, #T_a23ff_row17_col6, #T_a23ff_row18_col0, #T_a23ff_row18_col1, #T_a23ff_row18_col2, #T_a23ff_row18_col3, #T_a23ff_row18_col4, #T_a23ff_row18_col5, #T_a23ff_row18_col6 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_a23ff_row0_col4, #T_a23ff_row1_col1, #T_a23ff_row1_col2, #T_a23ff_row1_col3, #T_a23ff_row4_col6, #T_a23ff_row9_col5 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_a23ff_row0_col7, #T_a23ff_row1_col7, #T_a23ff_row2_col7, #T_a23ff_row3_col7, #T_a23ff_row4_col7, #T_a23ff_row5_col7, #T_a23ff_row6_col7, #T_a23ff_row7_col7, #T_a23ff_row8_col7, #T_a23ff_row9_col7, #T_a23ff_row10_col7, #T_a23ff_row11_col7, #T_a23ff_row12_col7, #T_a23ff_row14_col7, #T_a23ff_row15_col7, #T_a23ff_row16_col7, #T_a23ff_row17_col7, #T_a23ff_row18_col7 {\n",
       "  text-align: left;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "#T_a23ff_row13_col7 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a23ff_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th class=\"col_heading level0 col1\" >MAE</th>\n",
       "      <th class=\"col_heading level0 col2\" >MSE</th>\n",
       "      <th class=\"col_heading level0 col3\" >RMSE</th>\n",
       "      <th class=\"col_heading level0 col4\" >R2</th>\n",
       "      <th class=\"col_heading level0 col5\" >RMSLE</th>\n",
       "      <th class=\"col_heading level0 col6\" >MAPE</th>\n",
       "      <th class=\"col_heading level0 col7\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row0\" class=\"row_heading level0 row0\" >llar</th>\n",
       "      <td id=\"T_a23ff_row0_col0\" class=\"data row0 col0\" >Lasso Least Angle Regression</td>\n",
       "      <td id=\"T_a23ff_row0_col1\" class=\"data row0 col1\" >35.7658</td>\n",
       "      <td id=\"T_a23ff_row0_col2\" class=\"data row0 col2\" >2092.6260</td>\n",
       "      <td id=\"T_a23ff_row0_col3\" class=\"data row0 col3\" >43.6011</td>\n",
       "      <td id=\"T_a23ff_row0_col4\" class=\"data row0 col4\" >0.3510</td>\n",
       "      <td id=\"T_a23ff_row0_col5\" class=\"data row0 col5\" >1.5305</td>\n",
       "      <td id=\"T_a23ff_row0_col6\" class=\"data row0 col6\" >7.3770</td>\n",
       "      <td id=\"T_a23ff_row0_col7\" class=\"data row0 col7\" >0.1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row1\" class=\"row_heading level0 row1\" >catboost</th>\n",
       "      <td id=\"T_a23ff_row1_col0\" class=\"data row1 col0\" >CatBoost Regressor</td>\n",
       "      <td id=\"T_a23ff_row1_col1\" class=\"data row1 col1\" >32.1155</td>\n",
       "      <td id=\"T_a23ff_row1_col2\" class=\"data row1 col2\" >2055.6330</td>\n",
       "      <td id=\"T_a23ff_row1_col3\" class=\"data row1 col3\" >42.4693</td>\n",
       "      <td id=\"T_a23ff_row1_col4\" class=\"data row1 col4\" >0.3358</td>\n",
       "      <td id=\"T_a23ff_row1_col5\" class=\"data row1 col5\" >1.3558</td>\n",
       "      <td id=\"T_a23ff_row1_col6\" class=\"data row1 col6\" >5.7890</td>\n",
       "      <td id=\"T_a23ff_row1_col7\" class=\"data row1 col7\" >1.2540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row2\" class=\"row_heading level0 row2\" >omp</th>\n",
       "      <td id=\"T_a23ff_row2_col0\" class=\"data row2 col0\" >Orthogonal Matching Pursuit</td>\n",
       "      <td id=\"T_a23ff_row2_col1\" class=\"data row2 col1\" >34.7735</td>\n",
       "      <td id=\"T_a23ff_row2_col2\" class=\"data row2 col2\" >2099.0910</td>\n",
       "      <td id=\"T_a23ff_row2_col3\" class=\"data row2 col3\" >44.1072</td>\n",
       "      <td id=\"T_a23ff_row2_col4\" class=\"data row2 col4\" >0.2936</td>\n",
       "      <td id=\"T_a23ff_row2_col5\" class=\"data row2 col5\" >1.3323</td>\n",
       "      <td id=\"T_a23ff_row2_col6\" class=\"data row2 col6\" >5.6373</td>\n",
       "      <td id=\"T_a23ff_row2_col7\" class=\"data row2 col7\" >0.1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row3\" class=\"row_heading level0 row3\" >rf</th>\n",
       "      <td id=\"T_a23ff_row3_col0\" class=\"data row3 col0\" >Random Forest Regressor</td>\n",
       "      <td id=\"T_a23ff_row3_col1\" class=\"data row3 col1\" >33.2784</td>\n",
       "      <td id=\"T_a23ff_row3_col2\" class=\"data row3 col2\" >2209.7011</td>\n",
       "      <td id=\"T_a23ff_row3_col3\" class=\"data row3 col3\" >44.1515</td>\n",
       "      <td id=\"T_a23ff_row3_col4\" class=\"data row3 col4\" >0.2746</td>\n",
       "      <td id=\"T_a23ff_row3_col5\" class=\"data row3 col5\" >1.3634</td>\n",
       "      <td id=\"T_a23ff_row3_col6\" class=\"data row3 col6\" >6.3017</td>\n",
       "      <td id=\"T_a23ff_row3_col7\" class=\"data row3 col7\" >0.0660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row4\" class=\"row_heading level0 row4\" >lightgbm</th>\n",
       "      <td id=\"T_a23ff_row4_col0\" class=\"data row4 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_a23ff_row4_col1\" class=\"data row4 col1\" >33.0412</td>\n",
       "      <td id=\"T_a23ff_row4_col2\" class=\"data row4 col2\" >2295.1301</td>\n",
       "      <td id=\"T_a23ff_row4_col3\" class=\"data row4 col3\" >44.6611</td>\n",
       "      <td id=\"T_a23ff_row4_col4\" class=\"data row4 col4\" >0.2606</td>\n",
       "      <td id=\"T_a23ff_row4_col5\" class=\"data row4 col5\" >1.4124</td>\n",
       "      <td id=\"T_a23ff_row4_col6\" class=\"data row4 col6\" >5.3740</td>\n",
       "      <td id=\"T_a23ff_row4_col7\" class=\"data row4 col7\" >0.1110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row5\" class=\"row_heading level0 row5\" >et</th>\n",
       "      <td id=\"T_a23ff_row5_col0\" class=\"data row5 col0\" >Extra Trees Regressor</td>\n",
       "      <td id=\"T_a23ff_row5_col1\" class=\"data row5 col1\" >33.7103</td>\n",
       "      <td id=\"T_a23ff_row5_col2\" class=\"data row5 col2\" >2101.2911</td>\n",
       "      <td id=\"T_a23ff_row5_col3\" class=\"data row5 col3\" >43.9600</td>\n",
       "      <td id=\"T_a23ff_row5_col4\" class=\"data row5 col4\" >0.2474</td>\n",
       "      <td id=\"T_a23ff_row5_col5\" class=\"data row5 col5\" >1.3710</td>\n",
       "      <td id=\"T_a23ff_row5_col6\" class=\"data row5 col6\" >6.4835</td>\n",
       "      <td id=\"T_a23ff_row5_col7\" class=\"data row5 col7\" >0.0540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row6\" class=\"row_heading level0 row6\" >gbr</th>\n",
       "      <td id=\"T_a23ff_row6_col0\" class=\"data row6 col0\" >Gradient Boosting Regressor</td>\n",
       "      <td id=\"T_a23ff_row6_col1\" class=\"data row6 col1\" >33.1748</td>\n",
       "      <td id=\"T_a23ff_row6_col2\" class=\"data row6 col2\" >2372.0028</td>\n",
       "      <td id=\"T_a23ff_row6_col3\" class=\"data row6 col3\" >45.0098</td>\n",
       "      <td id=\"T_a23ff_row6_col4\" class=\"data row6 col4\" >0.2091</td>\n",
       "      <td id=\"T_a23ff_row6_col5\" class=\"data row6 col5\" >1.3109</td>\n",
       "      <td id=\"T_a23ff_row6_col6\" class=\"data row6 col6\" >6.8522</td>\n",
       "      <td id=\"T_a23ff_row6_col7\" class=\"data row6 col7\" >0.0150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row7\" class=\"row_heading level0 row7\" >ada</th>\n",
       "      <td id=\"T_a23ff_row7_col0\" class=\"data row7 col0\" >AdaBoost Regressor</td>\n",
       "      <td id=\"T_a23ff_row7_col1\" class=\"data row7 col1\" >36.1417</td>\n",
       "      <td id=\"T_a23ff_row7_col2\" class=\"data row7 col2\" >2386.2331</td>\n",
       "      <td id=\"T_a23ff_row7_col3\" class=\"data row7 col3\" >45.8706</td>\n",
       "      <td id=\"T_a23ff_row7_col4\" class=\"data row7 col4\" >0.2028</td>\n",
       "      <td id=\"T_a23ff_row7_col5\" class=\"data row7 col5\" >1.5883</td>\n",
       "      <td id=\"T_a23ff_row7_col6\" class=\"data row7 col6\" >8.6757</td>\n",
       "      <td id=\"T_a23ff_row7_col7\" class=\"data row7 col7\" >0.0180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row8\" class=\"row_heading level0 row8\" >lasso</th>\n",
       "      <td id=\"T_a23ff_row8_col0\" class=\"data row8 col0\" >Lasso Regression</td>\n",
       "      <td id=\"T_a23ff_row8_col1\" class=\"data row8 col1\" >35.8093</td>\n",
       "      <td id=\"T_a23ff_row8_col2\" class=\"data row8 col2\" >2270.4571</td>\n",
       "      <td id=\"T_a23ff_row8_col3\" class=\"data row8 col3\" >45.4098</td>\n",
       "      <td id=\"T_a23ff_row8_col4\" class=\"data row8 col4\" >0.1585</td>\n",
       "      <td id=\"T_a23ff_row8_col5\" class=\"data row8 col5\" >1.3808</td>\n",
       "      <td id=\"T_a23ff_row8_col6\" class=\"data row8 col6\" >6.1309</td>\n",
       "      <td id=\"T_a23ff_row8_col7\" class=\"data row8 col7\" >0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row9\" class=\"row_heading level0 row9\" >xgboost</th>\n",
       "      <td id=\"T_a23ff_row9_col0\" class=\"data row9 col0\" >Extreme Gradient Boosting</td>\n",
       "      <td id=\"T_a23ff_row9_col1\" class=\"data row9 col1\" >34.2073</td>\n",
       "      <td id=\"T_a23ff_row9_col2\" class=\"data row9 col2\" >2570.5121</td>\n",
       "      <td id=\"T_a23ff_row9_col3\" class=\"data row9 col3\" >47.1591</td>\n",
       "      <td id=\"T_a23ff_row9_col4\" class=\"data row9 col4\" >0.1227</td>\n",
       "      <td id=\"T_a23ff_row9_col5\" class=\"data row9 col5\" >1.2405</td>\n",
       "      <td id=\"T_a23ff_row9_col6\" class=\"data row9 col6\" >5.7157</td>\n",
       "      <td id=\"T_a23ff_row9_col7\" class=\"data row9 col7\" >53.4300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row10\" class=\"row_heading level0 row10\" >knn</th>\n",
       "      <td id=\"T_a23ff_row10_col0\" class=\"data row10 col0\" >K Neighbors Regressor</td>\n",
       "      <td id=\"T_a23ff_row10_col1\" class=\"data row10 col1\" >45.7938</td>\n",
       "      <td id=\"T_a23ff_row10_col2\" class=\"data row10 col2\" >3561.3714</td>\n",
       "      <td id=\"T_a23ff_row10_col3\" class=\"data row10 col3\" >57.5092</td>\n",
       "      <td id=\"T_a23ff_row10_col4\" class=\"data row10 col4\" >-0.2263</td>\n",
       "      <td id=\"T_a23ff_row10_col5\" class=\"data row10 col5\" >1.7238</td>\n",
       "      <td id=\"T_a23ff_row10_col6\" class=\"data row10 col6\" >14.1213</td>\n",
       "      <td id=\"T_a23ff_row10_col7\" class=\"data row10 col7\" >0.0090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row11\" class=\"row_heading level0 row11\" >dt</th>\n",
       "      <td id=\"T_a23ff_row11_col0\" class=\"data row11 col0\" >Decision Tree Regressor</td>\n",
       "      <td id=\"T_a23ff_row11_col1\" class=\"data row11 col1\" >46.3515</td>\n",
       "      <td id=\"T_a23ff_row11_col2\" class=\"data row11 col2\" >4583.7518</td>\n",
       "      <td id=\"T_a23ff_row11_col3\" class=\"data row11 col3\" >63.7920</td>\n",
       "      <td id=\"T_a23ff_row11_col4\" class=\"data row11 col4\" >-0.7558</td>\n",
       "      <td id=\"T_a23ff_row11_col5\" class=\"data row11 col5\" >1.3828</td>\n",
       "      <td id=\"T_a23ff_row11_col6\" class=\"data row11 col6\" >5.6954</td>\n",
       "      <td id=\"T_a23ff_row11_col7\" class=\"data row11 col7\" >0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row12\" class=\"row_heading level0 row12\" >huber</th>\n",
       "      <td id=\"T_a23ff_row12_col0\" class=\"data row12 col0\" >Huber Regressor</td>\n",
       "      <td id=\"T_a23ff_row12_col1\" class=\"data row12 col1\" >66.3258</td>\n",
       "      <td id=\"T_a23ff_row12_col2\" class=\"data row12 col2\" >14816.9784</td>\n",
       "      <td id=\"T_a23ff_row12_col3\" class=\"data row12 col3\" >96.9916</td>\n",
       "      <td id=\"T_a23ff_row12_col4\" class=\"data row12 col4\" >-10.4528</td>\n",
       "      <td id=\"T_a23ff_row12_col5\" class=\"data row12 col5\" >2.1090</td>\n",
       "      <td id=\"T_a23ff_row12_col6\" class=\"data row12 col6\" >24.1739</td>\n",
       "      <td id=\"T_a23ff_row12_col7\" class=\"data row12 col7\" >0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row13\" class=\"row_heading level0 row13\" >br</th>\n",
       "      <td id=\"T_a23ff_row13_col0\" class=\"data row13 col0\" >Bayesian Ridge</td>\n",
       "      <td id=\"T_a23ff_row13_col1\" class=\"data row13 col1\" >53.8333</td>\n",
       "      <td id=\"T_a23ff_row13_col2\" class=\"data row13 col2\" >44208.4241</td>\n",
       "      <td id=\"T_a23ff_row13_col3\" class=\"data row13 col3\" >106.7410</td>\n",
       "      <td id=\"T_a23ff_row13_col4\" class=\"data row13 col4\" >-44.3610</td>\n",
       "      <td id=\"T_a23ff_row13_col5\" class=\"data row13 col5\" >1.5915</td>\n",
       "      <td id=\"T_a23ff_row13_col6\" class=\"data row13 col6\" >15.2253</td>\n",
       "      <td id=\"T_a23ff_row13_col7\" class=\"data row13 col7\" >0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row14\" class=\"row_heading level0 row14\" >par</th>\n",
       "      <td id=\"T_a23ff_row14_col0\" class=\"data row14 col0\" >Passive Aggressive Regressor</td>\n",
       "      <td id=\"T_a23ff_row14_col1\" class=\"data row14 col1\" >163.4234</td>\n",
       "      <td id=\"T_a23ff_row14_col2\" class=\"data row14 col2\" >902737.9730</td>\n",
       "      <td id=\"T_a23ff_row14_col3\" class=\"data row14 col3\" >391.8883</td>\n",
       "      <td id=\"T_a23ff_row14_col4\" class=\"data row14 col4\" >-950.7012</td>\n",
       "      <td id=\"T_a23ff_row14_col5\" class=\"data row14 col5\" >2.3293</td>\n",
       "      <td id=\"T_a23ff_row14_col6\" class=\"data row14 col6\" >65.1600</td>\n",
       "      <td id=\"T_a23ff_row14_col7\" class=\"data row14 col7\" >0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row15\" class=\"row_heading level0 row15\" >en</th>\n",
       "      <td id=\"T_a23ff_row15_col0\" class=\"data row15 col0\" >Elastic Net</td>\n",
       "      <td id=\"T_a23ff_row15_col1\" class=\"data row15 col1\" >2272.5155</td>\n",
       "      <td id=\"T_a23ff_row15_col2\" class=\"data row15 col2\" >700632322.4450</td>\n",
       "      <td id=\"T_a23ff_row15_col3\" class=\"data row15 col3\" >8411.6807</td>\n",
       "      <td id=\"T_a23ff_row15_col4\" class=\"data row15 col4\" >-744626.9995</td>\n",
       "      <td id=\"T_a23ff_row15_col5\" class=\"data row15 col5\" >1.5960</td>\n",
       "      <td id=\"T_a23ff_row15_col6\" class=\"data row15 col6\" >1071.2204</td>\n",
       "      <td id=\"T_a23ff_row15_col7\" class=\"data row15 col7\" >0.1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row16\" class=\"row_heading level0 row16\" >ridge</th>\n",
       "      <td id=\"T_a23ff_row16_col0\" class=\"data row16 col0\" >Ridge Regression</td>\n",
       "      <td id=\"T_a23ff_row16_col1\" class=\"data row16 col1\" >14823.9408</td>\n",
       "      <td id=\"T_a23ff_row16_col2\" class=\"data row16 col2\" >30615138022.5054</td>\n",
       "      <td id=\"T_a23ff_row16_col3\" class=\"data row16 col3\" >55373.0329</td>\n",
       "      <td id=\"T_a23ff_row16_col4\" class=\"data row16 col4\" >-32537656.0975</td>\n",
       "      <td id=\"T_a23ff_row16_col5\" class=\"data row16 col5\" >1.6592</td>\n",
       "      <td id=\"T_a23ff_row16_col6\" class=\"data row16 col6\" >7047.4501</td>\n",
       "      <td id=\"T_a23ff_row16_col7\" class=\"data row16 col7\" >0.1540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row17\" class=\"row_heading level0 row17\" >lr</th>\n",
       "      <td id=\"T_a23ff_row17_col0\" class=\"data row17 col0\" >Linear Regression</td>\n",
       "      <td id=\"T_a23ff_row17_col1\" class=\"data row17 col1\" >15315.1162</td>\n",
       "      <td id=\"T_a23ff_row17_col2\" class=\"data row17 col2\" >32682562896.2679</td>\n",
       "      <td id=\"T_a23ff_row17_col3\" class=\"data row17 col3\" >57210.7839</td>\n",
       "      <td id=\"T_a23ff_row17_col4\" class=\"data row17 col4\" >-34734907.8802</td>\n",
       "      <td id=\"T_a23ff_row17_col5\" class=\"data row17 col5\" >1.6563</td>\n",
       "      <td id=\"T_a23ff_row17_col6\" class=\"data row17 col6\" >7281.3178</td>\n",
       "      <td id=\"T_a23ff_row17_col7\" class=\"data row17 col7\" >0.2550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a23ff_level0_row18\" class=\"row_heading level0 row18\" >lar</th>\n",
       "      <td id=\"T_a23ff_row18_col0\" class=\"data row18 col0\" >Least Angle Regression</td>\n",
       "      <td id=\"T_a23ff_row18_col1\" class=\"data row18 col1\" >15325.5461</td>\n",
       "      <td id=\"T_a23ff_row18_col2\" class=\"data row18 col2\" >32727199527.4869</td>\n",
       "      <td id=\"T_a23ff_row18_col3\" class=\"data row18 col3\" >57249.8125</td>\n",
       "      <td id=\"T_a23ff_row18_col4\" class=\"data row18 col4\" >-34782348.3664</td>\n",
       "      <td id=\"T_a23ff_row18_col5\" class=\"data row18 col5\" >1.6564</td>\n",
       "      <td id=\"T_a23ff_row18_col6\" class=\"data row18 col6\" >7286.2866</td>\n",
       "      <td id=\"T_a23ff_row18_col7\" class=\"data row18 col7\" >0.1310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f55643a6400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True,\n",
       "          fit_path=True, jitter=None, max_iter=500, normalize=True,\n",
       "          positive=False, precompute='auto', random_state=2021, verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1e63977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3eedc_row10_col0, #T_3eedc_row10_col1, #T_3eedc_row10_col2, #T_3eedc_row10_col3, #T_3eedc_row10_col4, #T_3eedc_row10_col5 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3eedc_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >MAE</th>\n",
       "      <th class=\"col_heading level0 col1\" >MSE</th>\n",
       "      <th class=\"col_heading level0 col2\" >RMSE</th>\n",
       "      <th class=\"col_heading level0 col3\" >R2</th>\n",
       "      <th class=\"col_heading level0 col4\" >RMSLE</th>\n",
       "      <th class=\"col_heading level0 col5\" >MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3eedc_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3eedc_row0_col0\" class=\"data row0 col0\" >31.4369</td>\n",
       "      <td id=\"T_3eedc_row0_col1\" class=\"data row0 col1\" >1489.6373</td>\n",
       "      <td id=\"T_3eedc_row0_col2\" class=\"data row0 col2\" >38.5958</td>\n",
       "      <td id=\"T_3eedc_row0_col3\" class=\"data row0 col3\" >0.4721</td>\n",
       "      <td id=\"T_3eedc_row0_col4\" class=\"data row0 col4\" >1.8064</td>\n",
       "      <td id=\"T_3eedc_row0_col5\" class=\"data row0 col5\" >10.9134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3eedc_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_3eedc_row1_col0\" class=\"data row1 col0\" >22.6568</td>\n",
       "      <td id=\"T_3eedc_row1_col1\" class=\"data row1 col1\" >629.3163</td>\n",
       "      <td id=\"T_3eedc_row1_col2\" class=\"data row1 col2\" >25.0862</td>\n",
       "      <td id=\"T_3eedc_row1_col3\" class=\"data row1 col3\" >0.3312</td>\n",
       "      <td id=\"T_3eedc_row1_col4\" class=\"data row1 col4\" >1.3767</td>\n",
       "      <td id=\"T_3eedc_row1_col5\" class=\"data row1 col5\" >12.2635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3eedc_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_3eedc_row2_col0\" class=\"data row2 col0\" >50.6664</td>\n",
       "      <td id=\"T_3eedc_row2_col1\" class=\"data row2 col1\" >4303.3545</td>\n",
       "      <td id=\"T_3eedc_row2_col2\" class=\"data row2 col2\" >65.6000</td>\n",
       "      <td id=\"T_3eedc_row2_col3\" class=\"data row2 col3\" >0.3047</td>\n",
       "      <td id=\"T_3eedc_row2_col4\" class=\"data row2 col4\" >1.3488</td>\n",
       "      <td id=\"T_3eedc_row2_col5\" class=\"data row2 col5\" >4.4200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3eedc_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_3eedc_row3_col0\" class=\"data row3 col0\" >34.9598</td>\n",
       "      <td id=\"T_3eedc_row3_col1\" class=\"data row3 col1\" >1475.3856</td>\n",
       "      <td id=\"T_3eedc_row3_col2\" class=\"data row3 col2\" >38.4107</td>\n",
       "      <td id=\"T_3eedc_row3_col3\" class=\"data row3 col3\" >0.5715</td>\n",
       "      <td id=\"T_3eedc_row3_col4\" class=\"data row3 col4\" >1.6524</td>\n",
       "      <td id=\"T_3eedc_row3_col5\" class=\"data row3 col5\" >1.8862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3eedc_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_3eedc_row4_col0\" class=\"data row4 col0\" >26.4613</td>\n",
       "      <td id=\"T_3eedc_row4_col1\" class=\"data row4 col1\" >981.2931</td>\n",
       "      <td id=\"T_3eedc_row4_col2\" class=\"data row4 col2\" >31.3256</td>\n",
       "      <td id=\"T_3eedc_row4_col3\" class=\"data row4 col3\" >0.6842</td>\n",
       "      <td id=\"T_3eedc_row4_col4\" class=\"data row4 col4\" >1.7673</td>\n",
       "      <td id=\"T_3eedc_row4_col5\" class=\"data row4 col5\" >13.9654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3eedc_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_3eedc_row5_col0\" class=\"data row5 col0\" >32.2952</td>\n",
       "      <td id=\"T_3eedc_row5_col1\" class=\"data row5 col1\" >1272.6737</td>\n",
       "      <td id=\"T_3eedc_row5_col2\" class=\"data row5 col2\" >35.6746</td>\n",
       "      <td id=\"T_3eedc_row5_col3\" class=\"data row5 col3\" >0.4404</td>\n",
       "      <td id=\"T_3eedc_row5_col4\" class=\"data row5 col4\" >1.3980</td>\n",
       "      <td id=\"T_3eedc_row5_col5\" class=\"data row5 col5\" >2.2719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3eedc_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_3eedc_row6_col0\" class=\"data row6 col0\" >41.3435</td>\n",
       "      <td id=\"T_3eedc_row6_col1\" class=\"data row6 col1\" >2632.8702</td>\n",
       "      <td id=\"T_3eedc_row6_col2\" class=\"data row6 col2\" >51.3115</td>\n",
       "      <td id=\"T_3eedc_row6_col3\" class=\"data row6 col3\" >-0.2002</td>\n",
       "      <td id=\"T_3eedc_row6_col4\" class=\"data row6 col4\" >1.6030</td>\n",
       "      <td id=\"T_3eedc_row6_col5\" class=\"data row6 col5\" >12.1814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3eedc_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_3eedc_row7_col0\" class=\"data row7 col0\" >43.4044</td>\n",
       "      <td id=\"T_3eedc_row7_col1\" class=\"data row7 col1\" >2638.0360</td>\n",
       "      <td id=\"T_3eedc_row7_col2\" class=\"data row7 col2\" >51.3618</td>\n",
       "      <td id=\"T_3eedc_row7_col3\" class=\"data row7 col3\" >0.3284</td>\n",
       "      <td id=\"T_3eedc_row7_col4\" class=\"data row7 col4\" >1.2397</td>\n",
       "      <td id=\"T_3eedc_row7_col5\" class=\"data row7 col5\" >4.9074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3eedc_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_3eedc_row8_col0\" class=\"data row8 col0\" >24.9008</td>\n",
       "      <td id=\"T_3eedc_row8_col1\" class=\"data row8 col1\" >989.6410</td>\n",
       "      <td id=\"T_3eedc_row8_col2\" class=\"data row8 col2\" >31.4586</td>\n",
       "      <td id=\"T_3eedc_row8_col3\" class=\"data row8 col3\" >0.2046</td>\n",
       "      <td id=\"T_3eedc_row8_col4\" class=\"data row8 col4\" >1.4036</td>\n",
       "      <td id=\"T_3eedc_row8_col5\" class=\"data row8 col5\" >6.7346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3eedc_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_3eedc_row9_col0\" class=\"data row9 col0\" >49.5328</td>\n",
       "      <td id=\"T_3eedc_row9_col1\" class=\"data row9 col1\" >4514.0526</td>\n",
       "      <td id=\"T_3eedc_row9_col2\" class=\"data row9 col2\" >67.1867</td>\n",
       "      <td id=\"T_3eedc_row9_col3\" class=\"data row9 col3\" >0.3733</td>\n",
       "      <td id=\"T_3eedc_row9_col4\" class=\"data row9 col4\" >1.7095</td>\n",
       "      <td id=\"T_3eedc_row9_col5\" class=\"data row9 col5\" >4.2258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3eedc_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_3eedc_row10_col0\" class=\"data row10 col0\" >35.7658</td>\n",
       "      <td id=\"T_3eedc_row10_col1\" class=\"data row10 col1\" >2092.6260</td>\n",
       "      <td id=\"T_3eedc_row10_col2\" class=\"data row10 col2\" >43.6011</td>\n",
       "      <td id=\"T_3eedc_row10_col3\" class=\"data row10 col3\" >0.3510</td>\n",
       "      <td id=\"T_3eedc_row10_col4\" class=\"data row10 col4\" >1.5305</td>\n",
       "      <td id=\"T_3eedc_row10_col5\" class=\"data row10 col5\" >7.3770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3eedc_level0_row11\" class=\"row_heading level0 row11\" >SD</th>\n",
       "      <td id=\"T_3eedc_row11_col0\" class=\"data row11 col0\" >9.5337</td>\n",
       "      <td id=\"T_3eedc_row11_col1\" class=\"data row11 col1\" >1317.1855</td>\n",
       "      <td id=\"T_3eedc_row11_col2\" class=\"data row11 col2\" >13.8407</td>\n",
       "      <td id=\"T_3eedc_row11_col3\" class=\"data row11 col3\" >0.2261</td>\n",
       "      <td id=\"T_3eedc_row11_col4\" class=\"data row11 col4\" >0.1896</td>\n",
       "      <td id=\"T_3eedc_row11_col5\" class=\"data row11 col5\" >4.2927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f54cc656100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = create_model('llar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "729fc8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_17200_row10_col0, #T_17200_row10_col1, #T_17200_row10_col2, #T_17200_row10_col3, #T_17200_row10_col4, #T_17200_row10_col5 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_17200_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >MAE</th>\n",
       "      <th class=\"col_heading level0 col1\" >MSE</th>\n",
       "      <th class=\"col_heading level0 col2\" >RMSE</th>\n",
       "      <th class=\"col_heading level0 col3\" >R2</th>\n",
       "      <th class=\"col_heading level0 col4\" >RMSLE</th>\n",
       "      <th class=\"col_heading level0 col5\" >MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_17200_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_17200_row0_col0\" class=\"data row0 col0\" >31.4369</td>\n",
       "      <td id=\"T_17200_row0_col1\" class=\"data row0 col1\" >1489.6373</td>\n",
       "      <td id=\"T_17200_row0_col2\" class=\"data row0 col2\" >38.5958</td>\n",
       "      <td id=\"T_17200_row0_col3\" class=\"data row0 col3\" >0.4721</td>\n",
       "      <td id=\"T_17200_row0_col4\" class=\"data row0 col4\" >1.8064</td>\n",
       "      <td id=\"T_17200_row0_col5\" class=\"data row0 col5\" >10.9134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_17200_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_17200_row1_col0\" class=\"data row1 col0\" >22.6568</td>\n",
       "      <td id=\"T_17200_row1_col1\" class=\"data row1 col1\" >629.3163</td>\n",
       "      <td id=\"T_17200_row1_col2\" class=\"data row1 col2\" >25.0862</td>\n",
       "      <td id=\"T_17200_row1_col3\" class=\"data row1 col3\" >0.3312</td>\n",
       "      <td id=\"T_17200_row1_col4\" class=\"data row1 col4\" >1.3767</td>\n",
       "      <td id=\"T_17200_row1_col5\" class=\"data row1 col5\" >12.2635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_17200_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_17200_row2_col0\" class=\"data row2 col0\" >50.6664</td>\n",
       "      <td id=\"T_17200_row2_col1\" class=\"data row2 col1\" >4303.3545</td>\n",
       "      <td id=\"T_17200_row2_col2\" class=\"data row2 col2\" >65.6000</td>\n",
       "      <td id=\"T_17200_row2_col3\" class=\"data row2 col3\" >0.3047</td>\n",
       "      <td id=\"T_17200_row2_col4\" class=\"data row2 col4\" >1.3488</td>\n",
       "      <td id=\"T_17200_row2_col5\" class=\"data row2 col5\" >4.4200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_17200_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_17200_row3_col0\" class=\"data row3 col0\" >34.9598</td>\n",
       "      <td id=\"T_17200_row3_col1\" class=\"data row3 col1\" >1475.3856</td>\n",
       "      <td id=\"T_17200_row3_col2\" class=\"data row3 col2\" >38.4107</td>\n",
       "      <td id=\"T_17200_row3_col3\" class=\"data row3 col3\" >0.5715</td>\n",
       "      <td id=\"T_17200_row3_col4\" class=\"data row3 col4\" >1.6524</td>\n",
       "      <td id=\"T_17200_row3_col5\" class=\"data row3 col5\" >1.8862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_17200_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_17200_row4_col0\" class=\"data row4 col0\" >26.4613</td>\n",
       "      <td id=\"T_17200_row4_col1\" class=\"data row4 col1\" >981.2931</td>\n",
       "      <td id=\"T_17200_row4_col2\" class=\"data row4 col2\" >31.3256</td>\n",
       "      <td id=\"T_17200_row4_col3\" class=\"data row4 col3\" >0.6842</td>\n",
       "      <td id=\"T_17200_row4_col4\" class=\"data row4 col4\" >1.7673</td>\n",
       "      <td id=\"T_17200_row4_col5\" class=\"data row4 col5\" >13.9654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_17200_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_17200_row5_col0\" class=\"data row5 col0\" >32.2952</td>\n",
       "      <td id=\"T_17200_row5_col1\" class=\"data row5 col1\" >1272.6737</td>\n",
       "      <td id=\"T_17200_row5_col2\" class=\"data row5 col2\" >35.6746</td>\n",
       "      <td id=\"T_17200_row5_col3\" class=\"data row5 col3\" >0.4404</td>\n",
       "      <td id=\"T_17200_row5_col4\" class=\"data row5 col4\" >1.3980</td>\n",
       "      <td id=\"T_17200_row5_col5\" class=\"data row5 col5\" >2.2719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_17200_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_17200_row6_col0\" class=\"data row6 col0\" >41.3435</td>\n",
       "      <td id=\"T_17200_row6_col1\" class=\"data row6 col1\" >2632.8702</td>\n",
       "      <td id=\"T_17200_row6_col2\" class=\"data row6 col2\" >51.3115</td>\n",
       "      <td id=\"T_17200_row6_col3\" class=\"data row6 col3\" >-0.2002</td>\n",
       "      <td id=\"T_17200_row6_col4\" class=\"data row6 col4\" >1.6030</td>\n",
       "      <td id=\"T_17200_row6_col5\" class=\"data row6 col5\" >12.1814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_17200_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_17200_row7_col0\" class=\"data row7 col0\" >43.4044</td>\n",
       "      <td id=\"T_17200_row7_col1\" class=\"data row7 col1\" >2638.0360</td>\n",
       "      <td id=\"T_17200_row7_col2\" class=\"data row7 col2\" >51.3618</td>\n",
       "      <td id=\"T_17200_row7_col3\" class=\"data row7 col3\" >0.3284</td>\n",
       "      <td id=\"T_17200_row7_col4\" class=\"data row7 col4\" >1.2397</td>\n",
       "      <td id=\"T_17200_row7_col5\" class=\"data row7 col5\" >4.9074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_17200_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_17200_row8_col0\" class=\"data row8 col0\" >24.9008</td>\n",
       "      <td id=\"T_17200_row8_col1\" class=\"data row8 col1\" >989.6410</td>\n",
       "      <td id=\"T_17200_row8_col2\" class=\"data row8 col2\" >31.4586</td>\n",
       "      <td id=\"T_17200_row8_col3\" class=\"data row8 col3\" >0.2046</td>\n",
       "      <td id=\"T_17200_row8_col4\" class=\"data row8 col4\" >1.4036</td>\n",
       "      <td id=\"T_17200_row8_col5\" class=\"data row8 col5\" >6.7346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_17200_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_17200_row9_col0\" class=\"data row9 col0\" >49.5328</td>\n",
       "      <td id=\"T_17200_row9_col1\" class=\"data row9 col1\" >4514.0526</td>\n",
       "      <td id=\"T_17200_row9_col2\" class=\"data row9 col2\" >67.1867</td>\n",
       "      <td id=\"T_17200_row9_col3\" class=\"data row9 col3\" >0.3733</td>\n",
       "      <td id=\"T_17200_row9_col4\" class=\"data row9 col4\" >1.7095</td>\n",
       "      <td id=\"T_17200_row9_col5\" class=\"data row9 col5\" >4.2258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_17200_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_17200_row10_col0\" class=\"data row10 col0\" >35.7658</td>\n",
       "      <td id=\"T_17200_row10_col1\" class=\"data row10 col1\" >2092.6260</td>\n",
       "      <td id=\"T_17200_row10_col2\" class=\"data row10 col2\" >43.6011</td>\n",
       "      <td id=\"T_17200_row10_col3\" class=\"data row10 col3\" >0.3510</td>\n",
       "      <td id=\"T_17200_row10_col4\" class=\"data row10 col4\" >1.5305</td>\n",
       "      <td id=\"T_17200_row10_col5\" class=\"data row10 col5\" >7.3770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_17200_level0_row11\" class=\"row_heading level0 row11\" >SD</th>\n",
       "      <td id=\"T_17200_row11_col0\" class=\"data row11 col0\" >9.5337</td>\n",
       "      <td id=\"T_17200_row11_col1\" class=\"data row11 col1\" >1317.1855</td>\n",
       "      <td id=\"T_17200_row11_col2\" class=\"data row11 col2\" >13.8407</td>\n",
       "      <td id=\"T_17200_row11_col3\" class=\"data row11 col3\" >0.2261</td>\n",
       "      <td id=\"T_17200_row11_col4\" class=\"data row11 col4\" >0.1896</td>\n",
       "      <td id=\"T_17200_row11_col5\" class=\"data row11 col5\" >4.2927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f55e599a4c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n"
     ]
    }
   ],
   "source": [
    "model_tuned = tune_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "132db89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=2.77309e-10): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=3.970e+00, previous alpha=3.954e+00, with an active set of 21 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.58691e-11): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 54 iterations, alpha=3.919e+00, previous alpha=1.115e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n",
      "/home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:598: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 42 iterations, alpha=1.299e+00, previous alpha=1.253e+00, with an active set of 23 regressors.\n",
      "  warnings.warn('Early stopping the lars path, as the residues '\n"
     ]
    }
   ],
   "source": [
    "plot_model(model_tuned, plot='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ebebf07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1463d3597e4e0885cb2e65aa62709b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Plot Type:', icons=('',), options=(('Hyperparameters', 'param…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_model(model_tuned, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuned.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f9bfa2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_31386_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th class=\"col_heading level0 col1\" >MAE</th>\n",
       "      <th class=\"col_heading level0 col2\" >MSE</th>\n",
       "      <th class=\"col_heading level0 col3\" >RMSE</th>\n",
       "      <th class=\"col_heading level0 col4\" >R2</th>\n",
       "      <th class=\"col_heading level0 col5\" >RMSLE</th>\n",
       "      <th class=\"col_heading level0 col6\" >MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_31386_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_31386_row0_col0\" class=\"data row0 col0\" >Least Angle Regression</td>\n",
       "      <td id=\"T_31386_row0_col1\" class=\"data row0 col1\" >42.8385</td>\n",
       "      <td id=\"T_31386_row0_col2\" class=\"data row0 col2\" >4072.4712</td>\n",
       "      <td id=\"T_31386_row0_col3\" class=\"data row0 col3\" >63.8159</td>\n",
       "      <td id=\"T_31386_row0_col4\" class=\"data row0 col4\" >0.3662</td>\n",
       "      <td id=\"T_31386_row0_col5\" class=\"data row0 col5\" >1.8988</td>\n",
       "      <td id=\"T_31386_row0_col6\" class=\"data row0 col6\" >6.3342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f54cc1e9af0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_17</th>\n",
       "      <th>feat_18</th>\n",
       "      <th>feat_19</th>\n",
       "      <th>feat_20</th>\n",
       "      <th>feat_21</th>\n",
       "      <th>feat_22</th>\n",
       "      <th>feat_23</th>\n",
       "      <th>feat_24</th>\n",
       "      <th>PRECIPITACAO TOTAL MENSAL(mm)</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173.000000</td>\n",
       "      <td>233.899994</td>\n",
       "      <td>214.699997</td>\n",
       "      <td>248.100006</td>\n",
       "      <td>230.199997</td>\n",
       "      <td>235.300003</td>\n",
       "      <td>983.429016</td>\n",
       "      <td>983.982117</td>\n",
       "      <td>982.932251</td>\n",
       "      <td>983.381104</td>\n",
       "      <td>...</td>\n",
       "      <td>28.810968</td>\n",
       "      <td>28.422667</td>\n",
       "      <td>66.814514</td>\n",
       "      <td>58.294643</td>\n",
       "      <td>62.991936</td>\n",
       "      <td>62.408333</td>\n",
       "      <td>58.935486</td>\n",
       "      <td>49.691666</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>40.330205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304.700012</td>\n",
       "      <td>288.700012</td>\n",
       "      <td>233.399994</td>\n",
       "      <td>211.100006</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>167.199997</td>\n",
       "      <td>982.778503</td>\n",
       "      <td>984.843323</td>\n",
       "      <td>984.815063</td>\n",
       "      <td>984.636536</td>\n",
       "      <td>...</td>\n",
       "      <td>27.497240</td>\n",
       "      <td>25.387741</td>\n",
       "      <td>50.879032</td>\n",
       "      <td>51.700001</td>\n",
       "      <td>55.290321</td>\n",
       "      <td>56.451614</td>\n",
       "      <td>58.603447</td>\n",
       "      <td>68.604836</td>\n",
       "      <td>139.600006</td>\n",
       "      <td>114.257491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>196.199997</td>\n",
       "      <td>191.500000</td>\n",
       "      <td>235.399994</td>\n",
       "      <td>261.500000</td>\n",
       "      <td>246.199997</td>\n",
       "      <td>214.899994</td>\n",
       "      <td>983.191650</td>\n",
       "      <td>984.515076</td>\n",
       "      <td>986.176697</td>\n",
       "      <td>985.813965</td>\n",
       "      <td>...</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>27.176773</td>\n",
       "      <td>72.053574</td>\n",
       "      <td>78.266129</td>\n",
       "      <td>79.158333</td>\n",
       "      <td>69.056450</td>\n",
       "      <td>68.250000</td>\n",
       "      <td>72275.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52.508798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281.100006</td>\n",
       "      <td>261.200012</td>\n",
       "      <td>227.899994</td>\n",
       "      <td>237.600006</td>\n",
       "      <td>227.100006</td>\n",
       "      <td>202.600006</td>\n",
       "      <td>985.798889</td>\n",
       "      <td>985.546265</td>\n",
       "      <td>986.197449</td>\n",
       "      <td>985.703552</td>\n",
       "      <td>...</td>\n",
       "      <td>28.592258</td>\n",
       "      <td>27.454666</td>\n",
       "      <td>60.541668</td>\n",
       "      <td>60.443546</td>\n",
       "      <td>65.129028</td>\n",
       "      <td>60.080357</td>\n",
       "      <td>62.709679</td>\n",
       "      <td>74.508331</td>\n",
       "      <td>93.400002</td>\n",
       "      <td>87.522940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>191.500000</td>\n",
       "      <td>235.399994</td>\n",
       "      <td>261.500000</td>\n",
       "      <td>246.199997</td>\n",
       "      <td>214.899994</td>\n",
       "      <td>311.200012</td>\n",
       "      <td>984.515076</td>\n",
       "      <td>986.176697</td>\n",
       "      <td>985.813965</td>\n",
       "      <td>987.101135</td>\n",
       "      <td>...</td>\n",
       "      <td>27.176773</td>\n",
       "      <td>27.693548</td>\n",
       "      <td>78.266129</td>\n",
       "      <td>79.158333</td>\n",
       "      <td>69.056450</td>\n",
       "      <td>68.250000</td>\n",
       "      <td>72275.000000</td>\n",
       "      <td>61.911289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.620884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>262.899994</td>\n",
       "      <td>283.100006</td>\n",
       "      <td>294.600006</td>\n",
       "      <td>305.200012</td>\n",
       "      <td>251.300003</td>\n",
       "      <td>299.500000</td>\n",
       "      <td>987.298889</td>\n",
       "      <td>987.880615</td>\n",
       "      <td>988.211853</td>\n",
       "      <td>986.159973</td>\n",
       "      <td>...</td>\n",
       "      <td>28.188387</td>\n",
       "      <td>28.819332</td>\n",
       "      <td>63.410713</td>\n",
       "      <td>54.822582</td>\n",
       "      <td>51.185486</td>\n",
       "      <td>51.458332</td>\n",
       "      <td>62.854839</td>\n",
       "      <td>57225.000000</td>\n",
       "      <td>86.900002</td>\n",
       "      <td>33.103750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>284.500000</td>\n",
       "      <td>275.899994</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>304.700012</td>\n",
       "      <td>288.700012</td>\n",
       "      <td>233.399994</td>\n",
       "      <td>987.847290</td>\n",
       "      <td>985.295532</td>\n",
       "      <td>984.695557</td>\n",
       "      <td>982.778503</td>\n",
       "      <td>...</td>\n",
       "      <td>28334.000000</td>\n",
       "      <td>27.925161</td>\n",
       "      <td>54.379032</td>\n",
       "      <td>51.049999</td>\n",
       "      <td>49375.000000</td>\n",
       "      <td>50.879032</td>\n",
       "      <td>51.700001</td>\n",
       "      <td>55.290321</td>\n",
       "      <td>83.599998</td>\n",
       "      <td>70.396729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>203.399994</td>\n",
       "      <td>228.500000</td>\n",
       "      <td>252.500000</td>\n",
       "      <td>281.299988</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>263.100006</td>\n",
       "      <td>985.133362</td>\n",
       "      <td>985.018250</td>\n",
       "      <td>985.792053</td>\n",
       "      <td>986.761047</td>\n",
       "      <td>...</td>\n",
       "      <td>27.963226</td>\n",
       "      <td>28.110001</td>\n",
       "      <td>85.283333</td>\n",
       "      <td>85.056450</td>\n",
       "      <td>80.891670</td>\n",
       "      <td>69.189651</td>\n",
       "      <td>61.290321</td>\n",
       "      <td>65.341667</td>\n",
       "      <td>23.600000</td>\n",
       "      <td>29.332732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>268.899994</td>\n",
       "      <td>277.799988</td>\n",
       "      <td>254.500000</td>\n",
       "      <td>223.199997</td>\n",
       "      <td>227.100006</td>\n",
       "      <td>170.699997</td>\n",
       "      <td>986.837219</td>\n",
       "      <td>986.267761</td>\n",
       "      <td>985.094421</td>\n",
       "      <td>985.926880</td>\n",
       "      <td>...</td>\n",
       "      <td>27.428387</td>\n",
       "      <td>26.850000</td>\n",
       "      <td>58.348213</td>\n",
       "      <td>61.854839</td>\n",
       "      <td>61.283333</td>\n",
       "      <td>63.653225</td>\n",
       "      <td>70.298386</td>\n",
       "      <td>80.767860</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>102.326448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>289.200012</td>\n",
       "      <td>283.399994</td>\n",
       "      <td>304.600006</td>\n",
       "      <td>270.700012</td>\n",
       "      <td>259.000000</td>\n",
       "      <td>266.799988</td>\n",
       "      <td>986.936584</td>\n",
       "      <td>986.961121</td>\n",
       "      <td>985.962341</td>\n",
       "      <td>985.435547</td>\n",
       "      <td>...</td>\n",
       "      <td>28.496128</td>\n",
       "      <td>28.879999</td>\n",
       "      <td>51.620968</td>\n",
       "      <td>50.058334</td>\n",
       "      <td>50.846775</td>\n",
       "      <td>64275.000000</td>\n",
       "      <td>64.282257</td>\n",
       "      <td>63.225807</td>\n",
       "      <td>175.800003</td>\n",
       "      <td>56.455567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>273.600006</td>\n",
       "      <td>284.600006</td>\n",
       "      <td>312.299988</td>\n",
       "      <td>288.100006</td>\n",
       "      <td>257.600006</td>\n",
       "      <td>239.399994</td>\n",
       "      <td>987.873108</td>\n",
       "      <td>987.220459</td>\n",
       "      <td>985.533325</td>\n",
       "      <td>985.377808</td>\n",
       "      <td>...</td>\n",
       "      <td>29.118065</td>\n",
       "      <td>28.323872</td>\n",
       "      <td>56.400002</td>\n",
       "      <td>56.183334</td>\n",
       "      <td>52.959679</td>\n",
       "      <td>58.908333</td>\n",
       "      <td>56.629032</td>\n",
       "      <td>61.475807</td>\n",
       "      <td>122.900002</td>\n",
       "      <td>66.840665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>285.799988</td>\n",
       "      <td>260.100006</td>\n",
       "      <td>214.300003</td>\n",
       "      <td>201.100006</td>\n",
       "      <td>194.500000</td>\n",
       "      <td>247.500000</td>\n",
       "      <td>983.119995</td>\n",
       "      <td>982.908630</td>\n",
       "      <td>982.767761</td>\n",
       "      <td>982.094055</td>\n",
       "      <td>...</td>\n",
       "      <td>28.006453</td>\n",
       "      <td>27.440666</td>\n",
       "      <td>58.750000</td>\n",
       "      <td>57.822582</td>\n",
       "      <td>59.903225</td>\n",
       "      <td>62.258930</td>\n",
       "      <td>67.532257</td>\n",
       "      <td>71025.000000</td>\n",
       "      <td>166.699997</td>\n",
       "      <td>67.049286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>188.100006</td>\n",
       "      <td>157.199997</td>\n",
       "      <td>238.100006</td>\n",
       "      <td>217.199997</td>\n",
       "      <td>232.500000</td>\n",
       "      <td>267.200012</td>\n",
       "      <td>986.419373</td>\n",
       "      <td>986.251099</td>\n",
       "      <td>986.309082</td>\n",
       "      <td>988.285583</td>\n",
       "      <td>...</td>\n",
       "      <td>26.923870</td>\n",
       "      <td>27.468388</td>\n",
       "      <td>83.709679</td>\n",
       "      <td>85.608330</td>\n",
       "      <td>78.500000</td>\n",
       "      <td>69.449997</td>\n",
       "      <td>66.379028</td>\n",
       "      <td>57.233871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.145885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>288.700012</td>\n",
       "      <td>233.399994</td>\n",
       "      <td>211.100006</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>167.199997</td>\n",
       "      <td>179.300003</td>\n",
       "      <td>984.843323</td>\n",
       "      <td>984.815063</td>\n",
       "      <td>984.636536</td>\n",
       "      <td>985.549438</td>\n",
       "      <td>...</td>\n",
       "      <td>25.387741</td>\n",
       "      <td>24.843332</td>\n",
       "      <td>51.700001</td>\n",
       "      <td>55.290321</td>\n",
       "      <td>56.451614</td>\n",
       "      <td>58.603447</td>\n",
       "      <td>68.604836</td>\n",
       "      <td>65.966667</td>\n",
       "      <td>132.399994</td>\n",
       "      <td>102.403397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>274.899994</td>\n",
       "      <td>237.899994</td>\n",
       "      <td>279.600006</td>\n",
       "      <td>307.799988</td>\n",
       "      <td>304.200012</td>\n",
       "      <td>308.399994</td>\n",
       "      <td>986.390320</td>\n",
       "      <td>988345.000000</td>\n",
       "      <td>987.896179</td>\n",
       "      <td>988.849487</td>\n",
       "      <td>...</td>\n",
       "      <td>27728.000000</td>\n",
       "      <td>27.992258</td>\n",
       "      <td>65.161293</td>\n",
       "      <td>69.766670</td>\n",
       "      <td>67.991936</td>\n",
       "      <td>63.725807</td>\n",
       "      <td>64.949997</td>\n",
       "      <td>65.040321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.023252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>317.700012</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>271.100006</td>\n",
       "      <td>289.799988</td>\n",
       "      <td>170.600006</td>\n",
       "      <td>232.899994</td>\n",
       "      <td>984.437622</td>\n",
       "      <td>983.957764</td>\n",
       "      <td>982.363464</td>\n",
       "      <td>982.843018</td>\n",
       "      <td>...</td>\n",
       "      <td>27.842142</td>\n",
       "      <td>26.598064</td>\n",
       "      <td>50.153225</td>\n",
       "      <td>52.033333</td>\n",
       "      <td>54.153225</td>\n",
       "      <td>55.629032</td>\n",
       "      <td>62.464287</td>\n",
       "      <td>66.145164</td>\n",
       "      <td>135.399994</td>\n",
       "      <td>87.322210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>259.799988</td>\n",
       "      <td>279.700012</td>\n",
       "      <td>295.200012</td>\n",
       "      <td>306.100006</td>\n",
       "      <td>240.600006</td>\n",
       "      <td>268.500000</td>\n",
       "      <td>988.823059</td>\n",
       "      <td>987.632263</td>\n",
       "      <td>987.513306</td>\n",
       "      <td>986.635498</td>\n",
       "      <td>...</td>\n",
       "      <td>27.641333</td>\n",
       "      <td>27.969032</td>\n",
       "      <td>78.193550</td>\n",
       "      <td>65.790321</td>\n",
       "      <td>65.258331</td>\n",
       "      <td>65.080643</td>\n",
       "      <td>66.433334</td>\n",
       "      <td>65.854836</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>48.675951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>260.100006</td>\n",
       "      <td>214.300003</td>\n",
       "      <td>201.100006</td>\n",
       "      <td>194.500000</td>\n",
       "      <td>247.500000</td>\n",
       "      <td>222.399994</td>\n",
       "      <td>982.908630</td>\n",
       "      <td>982.767761</td>\n",
       "      <td>982.094055</td>\n",
       "      <td>983.184937</td>\n",
       "      <td>...</td>\n",
       "      <td>27.440666</td>\n",
       "      <td>25.941935</td>\n",
       "      <td>57.822582</td>\n",
       "      <td>59.903225</td>\n",
       "      <td>62.258930</td>\n",
       "      <td>67.532257</td>\n",
       "      <td>71025.000000</td>\n",
       "      <td>78.750000</td>\n",
       "      <td>58.200001</td>\n",
       "      <td>66.870437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>215.300003</td>\n",
       "      <td>264.200012</td>\n",
       "      <td>289.200012</td>\n",
       "      <td>283.399994</td>\n",
       "      <td>304.600006</td>\n",
       "      <td>270.700012</td>\n",
       "      <td>987.520020</td>\n",
       "      <td>987.897827</td>\n",
       "      <td>986.936584</td>\n",
       "      <td>986.961121</td>\n",
       "      <td>...</td>\n",
       "      <td>28.249031</td>\n",
       "      <td>28064.000000</td>\n",
       "      <td>66.358330</td>\n",
       "      <td>60.225807</td>\n",
       "      <td>51.620968</td>\n",
       "      <td>50.058334</td>\n",
       "      <td>50.846775</td>\n",
       "      <td>64275.000000</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>31.440966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>252.699997</td>\n",
       "      <td>231.600006</td>\n",
       "      <td>274.899994</td>\n",
       "      <td>237.899994</td>\n",
       "      <td>279.600006</td>\n",
       "      <td>307.799988</td>\n",
       "      <td>985.932251</td>\n",
       "      <td>985.622192</td>\n",
       "      <td>986.390320</td>\n",
       "      <td>988345.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>27.477419</td>\n",
       "      <td>27.750969</td>\n",
       "      <td>69.040321</td>\n",
       "      <td>66.491669</td>\n",
       "      <td>65.161293</td>\n",
       "      <td>69.766670</td>\n",
       "      <td>67.991936</td>\n",
       "      <td>63.725807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.397390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>157.199997</td>\n",
       "      <td>238.100006</td>\n",
       "      <td>217.199997</td>\n",
       "      <td>232.500000</td>\n",
       "      <td>267.200012</td>\n",
       "      <td>257.600006</td>\n",
       "      <td>986.251099</td>\n",
       "      <td>986.309082</td>\n",
       "      <td>988.285583</td>\n",
       "      <td>988.389221</td>\n",
       "      <td>...</td>\n",
       "      <td>27.468388</td>\n",
       "      <td>28.192667</td>\n",
       "      <td>85.608330</td>\n",
       "      <td>78.500000</td>\n",
       "      <td>69.449997</td>\n",
       "      <td>66.379028</td>\n",
       "      <td>57.233871</td>\n",
       "      <td>59.008331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.099659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>235.399994</td>\n",
       "      <td>261.500000</td>\n",
       "      <td>246.199997</td>\n",
       "      <td>214.899994</td>\n",
       "      <td>311.200012</td>\n",
       "      <td>282.899994</td>\n",
       "      <td>986.176697</td>\n",
       "      <td>985.813965</td>\n",
       "      <td>987.101135</td>\n",
       "      <td>986.692322</td>\n",
       "      <td>...</td>\n",
       "      <td>27.693548</td>\n",
       "      <td>27.863333</td>\n",
       "      <td>79.158333</td>\n",
       "      <td>69.056450</td>\n",
       "      <td>68.250000</td>\n",
       "      <td>72275.000000</td>\n",
       "      <td>61.911289</td>\n",
       "      <td>65.683334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.703259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>193.000000</td>\n",
       "      <td>241.600006</td>\n",
       "      <td>235.100006</td>\n",
       "      <td>284.500000</td>\n",
       "      <td>275.899994</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>986.359985</td>\n",
       "      <td>986.845154</td>\n",
       "      <td>988.130005</td>\n",
       "      <td>987.847290</td>\n",
       "      <td>...</td>\n",
       "      <td>27006.000000</td>\n",
       "      <td>27918.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>63.056454</td>\n",
       "      <td>64.958336</td>\n",
       "      <td>54.379032</td>\n",
       "      <td>51.049999</td>\n",
       "      <td>49375.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.685841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>240.600006</td>\n",
       "      <td>268.500000</td>\n",
       "      <td>231.199997</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>220.800003</td>\n",
       "      <td>203.399994</td>\n",
       "      <td>985.917786</td>\n",
       "      <td>985.867737</td>\n",
       "      <td>985.734619</td>\n",
       "      <td>985.065491</td>\n",
       "      <td>...</td>\n",
       "      <td>26.760645</td>\n",
       "      <td>26764.000000</td>\n",
       "      <td>66.433334</td>\n",
       "      <td>65.854836</td>\n",
       "      <td>75.346771</td>\n",
       "      <td>76.642860</td>\n",
       "      <td>83.072578</td>\n",
       "      <td>85.283333</td>\n",
       "      <td>130.899994</td>\n",
       "      <td>77.637458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>291.799988</td>\n",
       "      <td>255.899994</td>\n",
       "      <td>266.200012</td>\n",
       "      <td>276.600006</td>\n",
       "      <td>196.199997</td>\n",
       "      <td>191.500000</td>\n",
       "      <td>983.433350</td>\n",
       "      <td>984.386658</td>\n",
       "      <td>983.941956</td>\n",
       "      <td>982.916687</td>\n",
       "      <td>...</td>\n",
       "      <td>27.911428</td>\n",
       "      <td>26.878065</td>\n",
       "      <td>63.241936</td>\n",
       "      <td>66.150002</td>\n",
       "      <td>65.750000</td>\n",
       "      <td>66.209679</td>\n",
       "      <td>72.053574</td>\n",
       "      <td>78.266129</td>\n",
       "      <td>50.500000</td>\n",
       "      <td>97.217337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>237.899994</td>\n",
       "      <td>279.600006</td>\n",
       "      <td>307.799988</td>\n",
       "      <td>304.200012</td>\n",
       "      <td>308.399994</td>\n",
       "      <td>281.100006</td>\n",
       "      <td>988345.000000</td>\n",
       "      <td>987.896179</td>\n",
       "      <td>988.849487</td>\n",
       "      <td>988.266663</td>\n",
       "      <td>...</td>\n",
       "      <td>27.992258</td>\n",
       "      <td>28.305332</td>\n",
       "      <td>69.766670</td>\n",
       "      <td>67.991936</td>\n",
       "      <td>63.725807</td>\n",
       "      <td>64.949997</td>\n",
       "      <td>65.040321</td>\n",
       "      <td>60.541668</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>32.934660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>299.100006</td>\n",
       "      <td>313.500000</td>\n",
       "      <td>222.699997</td>\n",
       "      <td>271.100006</td>\n",
       "      <td>244.699997</td>\n",
       "      <td>210.100006</td>\n",
       "      <td>985.803345</td>\n",
       "      <td>985.255920</td>\n",
       "      <td>983.476685</td>\n",
       "      <td>983.303223</td>\n",
       "      <td>...</td>\n",
       "      <td>28.294193</td>\n",
       "      <td>28.184286</td>\n",
       "      <td>47075.000000</td>\n",
       "      <td>46.290321</td>\n",
       "      <td>49425.000000</td>\n",
       "      <td>49.814514</td>\n",
       "      <td>57.677418</td>\n",
       "      <td>59.357143</td>\n",
       "      <td>169.600006</td>\n",
       "      <td>91.687071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>175.800003</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>235.899994</td>\n",
       "      <td>248.600006</td>\n",
       "      <td>188.199997</td>\n",
       "      <td>251.699997</td>\n",
       "      <td>984.237915</td>\n",
       "      <td>983.850525</td>\n",
       "      <td>984.023315</td>\n",
       "      <td>984.916138</td>\n",
       "      <td>...</td>\n",
       "      <td>24.355333</td>\n",
       "      <td>24.703871</td>\n",
       "      <td>85.922417</td>\n",
       "      <td>84.024193</td>\n",
       "      <td>77.083336</td>\n",
       "      <td>73.596771</td>\n",
       "      <td>82625.000000</td>\n",
       "      <td>76.298386</td>\n",
       "      <td>27.799999</td>\n",
       "      <td>35.004238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>257.600006</td>\n",
       "      <td>239.399994</td>\n",
       "      <td>161.800003</td>\n",
       "      <td>218.600006</td>\n",
       "      <td>229.300003</td>\n",
       "      <td>219.699997</td>\n",
       "      <td>985.572815</td>\n",
       "      <td>985.864502</td>\n",
       "      <td>985.696411</td>\n",
       "      <td>985.684937</td>\n",
       "      <td>...</td>\n",
       "      <td>26.790001</td>\n",
       "      <td>26.566452</td>\n",
       "      <td>56.629032</td>\n",
       "      <td>61.475807</td>\n",
       "      <td>71.035713</td>\n",
       "      <td>77.758064</td>\n",
       "      <td>78.391670</td>\n",
       "      <td>77.717743</td>\n",
       "      <td>82.500000</td>\n",
       "      <td>70.676454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>210.100006</td>\n",
       "      <td>216.699997</td>\n",
       "      <td>219.100006</td>\n",
       "      <td>211.800003</td>\n",
       "      <td>273.600006</td>\n",
       "      <td>257.799988</td>\n",
       "      <td>983.265503</td>\n",
       "      <td>983.250549</td>\n",
       "      <td>984.659973</td>\n",
       "      <td>984.960205</td>\n",
       "      <td>...</td>\n",
       "      <td>26.350668</td>\n",
       "      <td>26.858709</td>\n",
       "      <td>59.357143</td>\n",
       "      <td>71.137100</td>\n",
       "      <td>67.800003</td>\n",
       "      <td>75.750000</td>\n",
       "      <td>58.791668</td>\n",
       "      <td>49.911289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.473692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>297.000000</td>\n",
       "      <td>304.700012</td>\n",
       "      <td>288.700012</td>\n",
       "      <td>233.399994</td>\n",
       "      <td>211.100006</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>984.695557</td>\n",
       "      <td>982.778503</td>\n",
       "      <td>984.843323</td>\n",
       "      <td>984.815063</td>\n",
       "      <td>...</td>\n",
       "      <td>27.649033</td>\n",
       "      <td>27.497240</td>\n",
       "      <td>49375.000000</td>\n",
       "      <td>50.879032</td>\n",
       "      <td>51.700001</td>\n",
       "      <td>55.290321</td>\n",
       "      <td>56.451614</td>\n",
       "      <td>58.603447</td>\n",
       "      <td>250.399994</td>\n",
       "      <td>83.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>283.100006</td>\n",
       "      <td>294.600006</td>\n",
       "      <td>305.200012</td>\n",
       "      <td>251.300003</td>\n",
       "      <td>299.500000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>987.880615</td>\n",
       "      <td>988.211853</td>\n",
       "      <td>986.159973</td>\n",
       "      <td>984.888184</td>\n",
       "      <td>...</td>\n",
       "      <td>28.819332</td>\n",
       "      <td>27.288387</td>\n",
       "      <td>54.822582</td>\n",
       "      <td>51.185486</td>\n",
       "      <td>51.458332</td>\n",
       "      <td>62.854839</td>\n",
       "      <td>57225.000000</td>\n",
       "      <td>73.120972</td>\n",
       "      <td>137.699997</td>\n",
       "      <td>88.361714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>211.100006</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>167.199997</td>\n",
       "      <td>179.300003</td>\n",
       "      <td>218.800003</td>\n",
       "      <td>214.399994</td>\n",
       "      <td>984.636536</td>\n",
       "      <td>985.549438</td>\n",
       "      <td>984.678467</td>\n",
       "      <td>984.477783</td>\n",
       "      <td>...</td>\n",
       "      <td>24.878710</td>\n",
       "      <td>24.330000</td>\n",
       "      <td>56.451614</td>\n",
       "      <td>58.603447</td>\n",
       "      <td>68.604836</td>\n",
       "      <td>65.966667</td>\n",
       "      <td>66.056450</td>\n",
       "      <td>63775.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>60.994997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>242.399994</td>\n",
       "      <td>212.100006</td>\n",
       "      <td>217.399994</td>\n",
       "      <td>200.699997</td>\n",
       "      <td>208.699997</td>\n",
       "      <td>215.300003</td>\n",
       "      <td>982.675537</td>\n",
       "      <td>985.116150</td>\n",
       "      <td>984.996765</td>\n",
       "      <td>985.894226</td>\n",
       "      <td>...</td>\n",
       "      <td>26.917419</td>\n",
       "      <td>25.990000</td>\n",
       "      <td>52.391666</td>\n",
       "      <td>67.032257</td>\n",
       "      <td>72.943550</td>\n",
       "      <td>74.491379</td>\n",
       "      <td>77.806450</td>\n",
       "      <td>77.983330</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>67.862797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>241.600006</td>\n",
       "      <td>235.100006</td>\n",
       "      <td>284.500000</td>\n",
       "      <td>275.899994</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>304.700012</td>\n",
       "      <td>986.845154</td>\n",
       "      <td>988.130005</td>\n",
       "      <td>987.847290</td>\n",
       "      <td>985.295532</td>\n",
       "      <td>...</td>\n",
       "      <td>27918.000000</td>\n",
       "      <td>28.101934</td>\n",
       "      <td>63.056454</td>\n",
       "      <td>64.958336</td>\n",
       "      <td>54.379032</td>\n",
       "      <td>51.049999</td>\n",
       "      <td>49375.000000</td>\n",
       "      <td>50.879032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.530082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>318.299988</td>\n",
       "      <td>301.600006</td>\n",
       "      <td>281.299988</td>\n",
       "      <td>224.100006</td>\n",
       "      <td>171.199997</td>\n",
       "      <td>186.600006</td>\n",
       "      <td>986.582825</td>\n",
       "      <td>984.491089</td>\n",
       "      <td>984.931213</td>\n",
       "      <td>986.753784</td>\n",
       "      <td>...</td>\n",
       "      <td>26.738571</td>\n",
       "      <td>25.836775</td>\n",
       "      <td>48.846775</td>\n",
       "      <td>53175.000000</td>\n",
       "      <td>54.637096</td>\n",
       "      <td>59.556454</td>\n",
       "      <td>69.223213</td>\n",
       "      <td>75.500000</td>\n",
       "      <td>320.700012</td>\n",
       "      <td>111.691988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>263.100006</td>\n",
       "      <td>291.799988</td>\n",
       "      <td>255.899994</td>\n",
       "      <td>266.200012</td>\n",
       "      <td>276.600006</td>\n",
       "      <td>196.199997</td>\n",
       "      <td>983.803345</td>\n",
       "      <td>983.433350</td>\n",
       "      <td>984.386658</td>\n",
       "      <td>983.941956</td>\n",
       "      <td>...</td>\n",
       "      <td>28.538710</td>\n",
       "      <td>27.911428</td>\n",
       "      <td>65.341667</td>\n",
       "      <td>63.241936</td>\n",
       "      <td>66.150002</td>\n",
       "      <td>65.750000</td>\n",
       "      <td>66.209679</td>\n",
       "      <td>72.053574</td>\n",
       "      <td>160.699997</td>\n",
       "      <td>86.164270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>271.100006</td>\n",
       "      <td>289.799988</td>\n",
       "      <td>170.600006</td>\n",
       "      <td>232.899994</td>\n",
       "      <td>197.800003</td>\n",
       "      <td>223.199997</td>\n",
       "      <td>982.363464</td>\n",
       "      <td>982.843018</td>\n",
       "      <td>983.209534</td>\n",
       "      <td>984.183899</td>\n",
       "      <td>...</td>\n",
       "      <td>26.139999</td>\n",
       "      <td>25.232258</td>\n",
       "      <td>54.153225</td>\n",
       "      <td>55.629032</td>\n",
       "      <td>62.464287</td>\n",
       "      <td>66.145164</td>\n",
       "      <td>70.583336</td>\n",
       "      <td>70.258064</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>78.183198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>299.500000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>192.600006</td>\n",
       "      <td>186.600006</td>\n",
       "      <td>223.300003</td>\n",
       "      <td>195.399994</td>\n",
       "      <td>984.513306</td>\n",
       "      <td>984.758057</td>\n",
       "      <td>985.546265</td>\n",
       "      <td>987.187744</td>\n",
       "      <td>...</td>\n",
       "      <td>25.494194</td>\n",
       "      <td>25484.000000</td>\n",
       "      <td>57225.000000</td>\n",
       "      <td>73.120972</td>\n",
       "      <td>78.854836</td>\n",
       "      <td>85.482140</td>\n",
       "      <td>81.838707</td>\n",
       "      <td>82925.000000</td>\n",
       "      <td>120.699997</td>\n",
       "      <td>91.092026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>216.699997</td>\n",
       "      <td>219.100006</td>\n",
       "      <td>211.800003</td>\n",
       "      <td>273.600006</td>\n",
       "      <td>257.799988</td>\n",
       "      <td>309.899994</td>\n",
       "      <td>983.250549</td>\n",
       "      <td>984.659973</td>\n",
       "      <td>984.960205</td>\n",
       "      <td>985.270020</td>\n",
       "      <td>...</td>\n",
       "      <td>26.858709</td>\n",
       "      <td>27.625160</td>\n",
       "      <td>71.137100</td>\n",
       "      <td>67.800003</td>\n",
       "      <td>75.750000</td>\n",
       "      <td>58.791668</td>\n",
       "      <td>49.911289</td>\n",
       "      <td>46.298386</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>10.159690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>304.600006</td>\n",
       "      <td>270.700012</td>\n",
       "      <td>259.000000</td>\n",
       "      <td>266.799988</td>\n",
       "      <td>147.399994</td>\n",
       "      <td>226.500000</td>\n",
       "      <td>985.962341</td>\n",
       "      <td>985.435547</td>\n",
       "      <td>987.069885</td>\n",
       "      <td>986.507507</td>\n",
       "      <td>...</td>\n",
       "      <td>27.020000</td>\n",
       "      <td>26.101290</td>\n",
       "      <td>50.846775</td>\n",
       "      <td>64275.000000</td>\n",
       "      <td>64.282257</td>\n",
       "      <td>63.225807</td>\n",
       "      <td>74.357140</td>\n",
       "      <td>74.169357</td>\n",
       "      <td>96.500000</td>\n",
       "      <td>86.139753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>179.300003</td>\n",
       "      <td>218.800003</td>\n",
       "      <td>214.399994</td>\n",
       "      <td>229.399994</td>\n",
       "      <td>269.200012</td>\n",
       "      <td>292.600006</td>\n",
       "      <td>984.477783</td>\n",
       "      <td>985.784973</td>\n",
       "      <td>987.751099</td>\n",
       "      <td>988.684937</td>\n",
       "      <td>...</td>\n",
       "      <td>26.438709</td>\n",
       "      <td>28.159332</td>\n",
       "      <td>65.966667</td>\n",
       "      <td>66.056450</td>\n",
       "      <td>63775.000000</td>\n",
       "      <td>59.612904</td>\n",
       "      <td>51.879032</td>\n",
       "      <td>46.549999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.901714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>289.399994</td>\n",
       "      <td>296.399994</td>\n",
       "      <td>308.200012</td>\n",
       "      <td>245.300003</td>\n",
       "      <td>279.299988</td>\n",
       "      <td>155.399994</td>\n",
       "      <td>987.888184</td>\n",
       "      <td>987.281128</td>\n",
       "      <td>986.075256</td>\n",
       "      <td>985.591125</td>\n",
       "      <td>...</td>\n",
       "      <td>28.549032</td>\n",
       "      <td>26.810324</td>\n",
       "      <td>51.209679</td>\n",
       "      <td>51.612068</td>\n",
       "      <td>55.443546</td>\n",
       "      <td>58.666668</td>\n",
       "      <td>72.362900</td>\n",
       "      <td>84.072578</td>\n",
       "      <td>197.600006</td>\n",
       "      <td>114.341114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>246.199997</td>\n",
       "      <td>214.899994</td>\n",
       "      <td>311.200012</td>\n",
       "      <td>282.899994</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>305.500000</td>\n",
       "      <td>987.101135</td>\n",
       "      <td>986.692322</td>\n",
       "      <td>984.421509</td>\n",
       "      <td>985.332214</td>\n",
       "      <td>...</td>\n",
       "      <td>28.410969</td>\n",
       "      <td>28.889334</td>\n",
       "      <td>68.250000</td>\n",
       "      <td>72275.000000</td>\n",
       "      <td>61.911289</td>\n",
       "      <td>65.683334</td>\n",
       "      <td>65.927422</td>\n",
       "      <td>64575.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>17.246861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>231.600006</td>\n",
       "      <td>274.899994</td>\n",
       "      <td>237.899994</td>\n",
       "      <td>279.600006</td>\n",
       "      <td>307.799988</td>\n",
       "      <td>304.200012</td>\n",
       "      <td>985.622192</td>\n",
       "      <td>986.390320</td>\n",
       "      <td>988345.000000</td>\n",
       "      <td>987.896179</td>\n",
       "      <td>...</td>\n",
       "      <td>27.750969</td>\n",
       "      <td>27728.000000</td>\n",
       "      <td>66.491669</td>\n",
       "      <td>65.161293</td>\n",
       "      <td>69.766670</td>\n",
       "      <td>67.991936</td>\n",
       "      <td>63.725807</td>\n",
       "      <td>64.949997</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>18.966188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>278.200012</td>\n",
       "      <td>290.299988</td>\n",
       "      <td>239.899994</td>\n",
       "      <td>191.699997</td>\n",
       "      <td>252.699997</td>\n",
       "      <td>231.600006</td>\n",
       "      <td>987.074463</td>\n",
       "      <td>986.974182</td>\n",
       "      <td>987.705139</td>\n",
       "      <td>986.696533</td>\n",
       "      <td>...</td>\n",
       "      <td>27.322580</td>\n",
       "      <td>27916.000000</td>\n",
       "      <td>70.266670</td>\n",
       "      <td>67.919357</td>\n",
       "      <td>71.822578</td>\n",
       "      <td>73.870689</td>\n",
       "      <td>69.040321</td>\n",
       "      <td>66.491669</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>72.803636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>218.600006</td>\n",
       "      <td>229.300003</td>\n",
       "      <td>219.699997</td>\n",
       "      <td>246.800003</td>\n",
       "      <td>274.100006</td>\n",
       "      <td>289.399994</td>\n",
       "      <td>985.684937</td>\n",
       "      <td>986.080017</td>\n",
       "      <td>986.874207</td>\n",
       "      <td>987.374451</td>\n",
       "      <td>...</td>\n",
       "      <td>26.746452</td>\n",
       "      <td>27.701935</td>\n",
       "      <td>77.758064</td>\n",
       "      <td>78.391670</td>\n",
       "      <td>77.717743</td>\n",
       "      <td>72.608330</td>\n",
       "      <td>52.056454</td>\n",
       "      <td>51.209679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.280452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>237.000000</td>\n",
       "      <td>235.899994</td>\n",
       "      <td>248.600006</td>\n",
       "      <td>188.199997</td>\n",
       "      <td>251.699997</td>\n",
       "      <td>295.799988</td>\n",
       "      <td>983.850525</td>\n",
       "      <td>984.023315</td>\n",
       "      <td>984.916138</td>\n",
       "      <td>986.649414</td>\n",
       "      <td>...</td>\n",
       "      <td>24.703871</td>\n",
       "      <td>26.578711</td>\n",
       "      <td>84.024193</td>\n",
       "      <td>77.083336</td>\n",
       "      <td>73.596771</td>\n",
       "      <td>82625.000000</td>\n",
       "      <td>76.298386</td>\n",
       "      <td>63.774193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.415277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>239.899994</td>\n",
       "      <td>191.699997</td>\n",
       "      <td>252.699997</td>\n",
       "      <td>231.600006</td>\n",
       "      <td>274.899994</td>\n",
       "      <td>237.899994</td>\n",
       "      <td>987.705139</td>\n",
       "      <td>986.696533</td>\n",
       "      <td>985.932251</td>\n",
       "      <td>985.622192</td>\n",
       "      <td>...</td>\n",
       "      <td>28.040646</td>\n",
       "      <td>27542.000000</td>\n",
       "      <td>71.822578</td>\n",
       "      <td>73.870689</td>\n",
       "      <td>69.040321</td>\n",
       "      <td>66.491669</td>\n",
       "      <td>65.161293</td>\n",
       "      <td>69.766670</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>50.311065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>289.799988</td>\n",
       "      <td>170.600006</td>\n",
       "      <td>232.899994</td>\n",
       "      <td>197.800003</td>\n",
       "      <td>223.199997</td>\n",
       "      <td>215.300003</td>\n",
       "      <td>982.843018</td>\n",
       "      <td>983.209534</td>\n",
       "      <td>984.183899</td>\n",
       "      <td>984.468872</td>\n",
       "      <td>...</td>\n",
       "      <td>25.232258</td>\n",
       "      <td>24.765333</td>\n",
       "      <td>55.629032</td>\n",
       "      <td>62.464287</td>\n",
       "      <td>66.145164</td>\n",
       "      <td>70.583336</td>\n",
       "      <td>70.258064</td>\n",
       "      <td>66.358330</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>75.827174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>305.200012</td>\n",
       "      <td>251.300003</td>\n",
       "      <td>299.500000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>192.600006</td>\n",
       "      <td>186.600006</td>\n",
       "      <td>986.159973</td>\n",
       "      <td>984.888184</td>\n",
       "      <td>984.513306</td>\n",
       "      <td>984.758057</td>\n",
       "      <td>...</td>\n",
       "      <td>25.906452</td>\n",
       "      <td>25.684286</td>\n",
       "      <td>51.458332</td>\n",
       "      <td>62.854839</td>\n",
       "      <td>57225.000000</td>\n",
       "      <td>73.120972</td>\n",
       "      <td>78.854836</td>\n",
       "      <td>85.482140</td>\n",
       "      <td>156.600006</td>\n",
       "      <td>103.010815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>212.100006</td>\n",
       "      <td>217.399994</td>\n",
       "      <td>200.699997</td>\n",
       "      <td>208.699997</td>\n",
       "      <td>215.300003</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>985.116150</td>\n",
       "      <td>984.996765</td>\n",
       "      <td>985.894226</td>\n",
       "      <td>985.081726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.990000</td>\n",
       "      <td>25.692259</td>\n",
       "      <td>67.032257</td>\n",
       "      <td>72.943550</td>\n",
       "      <td>74.491379</td>\n",
       "      <td>77.806450</td>\n",
       "      <td>77.983330</td>\n",
       "      <td>78.540321</td>\n",
       "      <td>95.099998</td>\n",
       "      <td>47.868647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>188.000000</td>\n",
       "      <td>220.800003</td>\n",
       "      <td>203.399994</td>\n",
       "      <td>228.500000</td>\n",
       "      <td>252.500000</td>\n",
       "      <td>281.299988</td>\n",
       "      <td>985.065491</td>\n",
       "      <td>985.559143</td>\n",
       "      <td>985.133362</td>\n",
       "      <td>985.018250</td>\n",
       "      <td>...</td>\n",
       "      <td>26.687332</td>\n",
       "      <td>27.129032</td>\n",
       "      <td>76.642860</td>\n",
       "      <td>83.072578</td>\n",
       "      <td>85.283333</td>\n",
       "      <td>85.056450</td>\n",
       "      <td>80.891670</td>\n",
       "      <td>69.189651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.047291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>171.199997</td>\n",
       "      <td>186.600006</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>136.399994</td>\n",
       "      <td>222.199997</td>\n",
       "      <td>223.300003</td>\n",
       "      <td>987.038086</td>\n",
       "      <td>986.553772</td>\n",
       "      <td>987.556641</td>\n",
       "      <td>987.497864</td>\n",
       "      <td>...</td>\n",
       "      <td>24.670000</td>\n",
       "      <td>24.801291</td>\n",
       "      <td>69.223213</td>\n",
       "      <td>75.500000</td>\n",
       "      <td>74.916664</td>\n",
       "      <td>77.612900</td>\n",
       "      <td>76.641670</td>\n",
       "      <td>73.620972</td>\n",
       "      <td>54.099998</td>\n",
       "      <td>42.577139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>214.699997</td>\n",
       "      <td>248.100006</td>\n",
       "      <td>230.199997</td>\n",
       "      <td>235.300003</td>\n",
       "      <td>248.199997</td>\n",
       "      <td>281.200012</td>\n",
       "      <td>982.932251</td>\n",
       "      <td>983.381104</td>\n",
       "      <td>984.904297</td>\n",
       "      <td>986.508911</td>\n",
       "      <td>...</td>\n",
       "      <td>28.205807</td>\n",
       "      <td>28.108387</td>\n",
       "      <td>62.991936</td>\n",
       "      <td>62.408333</td>\n",
       "      <td>58.935486</td>\n",
       "      <td>49.691666</td>\n",
       "      <td>49.967743</td>\n",
       "      <td>50.209679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.461373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>274.100006</td>\n",
       "      <td>289.399994</td>\n",
       "      <td>296.399994</td>\n",
       "      <td>308.200012</td>\n",
       "      <td>245.300003</td>\n",
       "      <td>279.299988</td>\n",
       "      <td>988.201050</td>\n",
       "      <td>987.888184</td>\n",
       "      <td>987.281128</td>\n",
       "      <td>986.075256</td>\n",
       "      <td>...</td>\n",
       "      <td>28.949333</td>\n",
       "      <td>28.549032</td>\n",
       "      <td>52.056454</td>\n",
       "      <td>51.209679</td>\n",
       "      <td>51.612068</td>\n",
       "      <td>55.443546</td>\n",
       "      <td>58.666668</td>\n",
       "      <td>72.362900</td>\n",
       "      <td>305.399994</td>\n",
       "      <td>47.306728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>246.800003</td>\n",
       "      <td>274.100006</td>\n",
       "      <td>289.399994</td>\n",
       "      <td>296.399994</td>\n",
       "      <td>308.200012</td>\n",
       "      <td>245.300003</td>\n",
       "      <td>987.374451</td>\n",
       "      <td>988.201050</td>\n",
       "      <td>987.888184</td>\n",
       "      <td>987.281128</td>\n",
       "      <td>...</td>\n",
       "      <td>28.494194</td>\n",
       "      <td>28.949333</td>\n",
       "      <td>72.608330</td>\n",
       "      <td>52.056454</td>\n",
       "      <td>51.209679</td>\n",
       "      <td>51.612068</td>\n",
       "      <td>55.443546</td>\n",
       "      <td>58.666668</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>53.355519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        feat_1      feat_2      feat_3      feat_4      feat_5      feat_6  \\\n",
       "0   173.000000  233.899994  214.699997  248.100006  230.199997  235.300003   \n",
       "1   304.700012  288.700012  233.399994  211.100006  227.000000  167.199997   \n",
       "2   196.199997  191.500000  235.399994  261.500000  246.199997  214.899994   \n",
       "3   281.100006  261.200012  227.899994  237.600006  227.100006  202.600006   \n",
       "4   191.500000  235.399994  261.500000  246.199997  214.899994  311.200012   \n",
       "5   262.899994  283.100006  294.600006  305.200012  251.300003  299.500000   \n",
       "6   284.500000  275.899994  297.000000  304.700012  288.700012  233.399994   \n",
       "7   203.399994  228.500000  252.500000  281.299988  305.000000  263.100006   \n",
       "8   268.899994  277.799988  254.500000  223.199997  227.100006  170.699997   \n",
       "9   289.200012  283.399994  304.600006  270.700012  259.000000  266.799988   \n",
       "10  273.600006  284.600006  312.299988  288.100006  257.600006  239.399994   \n",
       "11  285.799988  260.100006  214.300003  201.100006  194.500000  247.500000   \n",
       "12  188.100006  157.199997  238.100006  217.199997  232.500000  267.200012   \n",
       "13  288.700012  233.399994  211.100006  227.000000  167.199997  179.300003   \n",
       "14  274.899994  237.899994  279.600006  307.799988  304.200012  308.399994   \n",
       "15  317.700012  299.000000  271.100006  289.799988  170.600006  232.899994   \n",
       "16  259.799988  279.700012  295.200012  306.100006  240.600006  268.500000   \n",
       "17  260.100006  214.300003  201.100006  194.500000  247.500000  222.399994   \n",
       "18  215.300003  264.200012  289.200012  283.399994  304.600006  270.700012   \n",
       "19  252.699997  231.600006  274.899994  237.899994  279.600006  307.799988   \n",
       "20  157.199997  238.100006  217.199997  232.500000  267.200012  257.600006   \n",
       "21  235.399994  261.500000  246.199997  214.899994  311.200012  282.899994   \n",
       "22  193.000000  241.600006  235.100006  284.500000  275.899994  297.000000   \n",
       "23  240.600006  268.500000  231.199997  188.000000  220.800003  203.399994   \n",
       "24  291.799988  255.899994  266.200012  276.600006  196.199997  191.500000   \n",
       "25  237.899994  279.600006  307.799988  304.200012  308.399994  281.100006   \n",
       "26  299.100006  313.500000  222.699997  271.100006  244.699997  210.100006   \n",
       "27  175.800003  237.000000  235.899994  248.600006  188.199997  251.699997   \n",
       "28  257.600006  239.399994  161.800003  218.600006  229.300003  219.699997   \n",
       "29  210.100006  216.699997  219.100006  211.800003  273.600006  257.799988   \n",
       "30  297.000000  304.700012  288.700012  233.399994  211.100006  227.000000   \n",
       "31  283.100006  294.600006  305.200012  251.300003  299.500000  200.000000   \n",
       "32  211.100006  227.000000  167.199997  179.300003  218.800003  214.399994   \n",
       "33  242.399994  212.100006  217.399994  200.699997  208.699997  215.300003   \n",
       "34  241.600006  235.100006  284.500000  275.899994  297.000000  304.700012   \n",
       "35  318.299988  301.600006  281.299988  224.100006  171.199997  186.600006   \n",
       "36  263.100006  291.799988  255.899994  266.200012  276.600006  196.199997   \n",
       "37  271.100006  289.799988  170.600006  232.899994  197.800003  223.199997   \n",
       "38  299.500000  200.000000  192.600006  186.600006  223.300003  195.399994   \n",
       "39  216.699997  219.100006  211.800003  273.600006  257.799988  309.899994   \n",
       "40  304.600006  270.700012  259.000000  266.799988  147.399994  226.500000   \n",
       "41  179.300003  218.800003  214.399994  229.399994  269.200012  292.600006   \n",
       "42  289.399994  296.399994  308.200012  245.300003  279.299988  155.399994   \n",
       "43  246.199997  214.899994  311.200012  282.899994  304.000000  305.500000   \n",
       "44  231.600006  274.899994  237.899994  279.600006  307.799988  304.200012   \n",
       "45  278.200012  290.299988  239.899994  191.699997  252.699997  231.600006   \n",
       "46  218.600006  229.300003  219.699997  246.800003  274.100006  289.399994   \n",
       "47  237.000000  235.899994  248.600006  188.199997  251.699997  295.799988   \n",
       "48  239.899994  191.699997  252.699997  231.600006  274.899994  237.899994   \n",
       "49  289.799988  170.600006  232.899994  197.800003  223.199997  215.300003   \n",
       "50  305.200012  251.300003  299.500000  200.000000  192.600006  186.600006   \n",
       "51  212.100006  217.399994  200.699997  208.699997  215.300003  239.000000   \n",
       "52  188.000000  220.800003  203.399994  228.500000  252.500000  281.299988   \n",
       "53  171.199997  186.600006  158.000000  136.399994  222.199997  223.300003   \n",
       "54  214.699997  248.100006  230.199997  235.300003  248.199997  281.200012   \n",
       "55  274.100006  289.399994  296.399994  308.200012  245.300003  279.299988   \n",
       "56  246.800003  274.100006  289.399994  296.399994  308.200012  245.300003   \n",
       "\n",
       "           feat_7         feat_8         feat_9        feat_10  ...  \\\n",
       "0      983.429016     983.982117     982.932251     983.381104  ...   \n",
       "1      982.778503     984.843323     984.815063     984.636536  ...   \n",
       "2      983.191650     984.515076     986.176697     985.813965  ...   \n",
       "3      985.798889     985.546265     986.197449     985.703552  ...   \n",
       "4      984.515076     986.176697     985.813965     987.101135  ...   \n",
       "5      987.298889     987.880615     988.211853     986.159973  ...   \n",
       "6      987.847290     985.295532     984.695557     982.778503  ...   \n",
       "7      985.133362     985.018250     985.792053     986.761047  ...   \n",
       "8      986.837219     986.267761     985.094421     985.926880  ...   \n",
       "9      986.936584     986.961121     985.962341     985.435547  ...   \n",
       "10     987.873108     987.220459     985.533325     985.377808  ...   \n",
       "11     983.119995     982.908630     982.767761     982.094055  ...   \n",
       "12     986.419373     986.251099     986.309082     988.285583  ...   \n",
       "13     984.843323     984.815063     984.636536     985.549438  ...   \n",
       "14     986.390320  988345.000000     987.896179     988.849487  ...   \n",
       "15     984.437622     983.957764     982.363464     982.843018  ...   \n",
       "16     988.823059     987.632263     987.513306     986.635498  ...   \n",
       "17     982.908630     982.767761     982.094055     983.184937  ...   \n",
       "18     987.520020     987.897827     986.936584     986.961121  ...   \n",
       "19     985.932251     985.622192     986.390320  988345.000000  ...   \n",
       "20     986.251099     986.309082     988.285583     988.389221  ...   \n",
       "21     986.176697     985.813965     987.101135     986.692322  ...   \n",
       "22     986.359985     986.845154     988.130005     987.847290  ...   \n",
       "23     985.917786     985.867737     985.734619     985.065491  ...   \n",
       "24     983.433350     984.386658     983.941956     982.916687  ...   \n",
       "25  988345.000000     987.896179     988.849487     988.266663  ...   \n",
       "26     985.803345     985.255920     983.476685     983.303223  ...   \n",
       "27     984.237915     983.850525     984.023315     984.916138  ...   \n",
       "28     985.572815     985.864502     985.696411     985.684937  ...   \n",
       "29     983.265503     983.250549     984.659973     984.960205  ...   \n",
       "30     984.695557     982.778503     984.843323     984.815063  ...   \n",
       "31     987.880615     988.211853     986.159973     984.888184  ...   \n",
       "32     984.636536     985.549438     984.678467     984.477783  ...   \n",
       "33     982.675537     985.116150     984.996765     985.894226  ...   \n",
       "34     986.845154     988.130005     987.847290     985.295532  ...   \n",
       "35     986.582825     984.491089     984.931213     986.753784  ...   \n",
       "36     983.803345     983.433350     984.386658     983.941956  ...   \n",
       "37     982.363464     982.843018     983.209534     984.183899  ...   \n",
       "38     984.513306     984.758057     985.546265     987.187744  ...   \n",
       "39     983.250549     984.659973     984.960205     985.270020  ...   \n",
       "40     985.962341     985.435547     987.069885     986.507507  ...   \n",
       "41     984.477783     985.784973     987.751099     988.684937  ...   \n",
       "42     987.888184     987.281128     986.075256     985.591125  ...   \n",
       "43     987.101135     986.692322     984.421509     985.332214  ...   \n",
       "44     985.622192     986.390320  988345.000000     987.896179  ...   \n",
       "45     987.074463     986.974182     987.705139     986.696533  ...   \n",
       "46     985.684937     986.080017     986.874207     987.374451  ...   \n",
       "47     983.850525     984.023315     984.916138     986.649414  ...   \n",
       "48     987.705139     986.696533     985.932251     985.622192  ...   \n",
       "49     982.843018     983.209534     984.183899     984.468872  ...   \n",
       "50     986.159973     984.888184     984.513306     984.758057  ...   \n",
       "51     985.116150     984.996765     985.894226     985.081726  ...   \n",
       "52     985.065491     985.559143     985.133362     985.018250  ...   \n",
       "53     987.038086     986.553772     987.556641     987.497864  ...   \n",
       "54     982.932251     983.381104     984.904297     986.508911  ...   \n",
       "55     988.201050     987.888184     987.281128     986.075256  ...   \n",
       "56     987.374451     988.201050     987.888184     987.281128  ...   \n",
       "\n",
       "         feat_17       feat_18       feat_19       feat_20       feat_21  \\\n",
       "0      28.810968     28.422667     66.814514     58.294643     62.991936   \n",
       "1      27.497240     25.387741     50.879032     51.700001     55.290321   \n",
       "2      27.600000     27.176773     72.053574     78.266129     79.158333   \n",
       "3      28.592258     27.454666     60.541668     60.443546     65.129028   \n",
       "4      27.176773     27.693548     78.266129     79.158333     69.056450   \n",
       "5      28.188387     28.819332     63.410713     54.822582     51.185486   \n",
       "6   28334.000000     27.925161     54.379032     51.049999  49375.000000   \n",
       "7      27.963226     28.110001     85.283333     85.056450     80.891670   \n",
       "8      27.428387     26.850000     58.348213     61.854839     61.283333   \n",
       "9      28.496128     28.879999     51.620968     50.058334     50.846775   \n",
       "10     29.118065     28.323872     56.400002     56.183334     52.959679   \n",
       "11     28.006453     27.440666     58.750000     57.822582     59.903225   \n",
       "12     26.923870     27.468388     83.709679     85.608330     78.500000   \n",
       "13     25.387741     24.843332     51.700001     55.290321     56.451614   \n",
       "14  27728.000000     27.992258     65.161293     69.766670     67.991936   \n",
       "15     27.842142     26.598064     50.153225     52.033333     54.153225   \n",
       "16     27.641333     27.969032     78.193550     65.790321     65.258331   \n",
       "17     27.440666     25.941935     57.822582     59.903225     62.258930   \n",
       "18     28.249031  28064.000000     66.358330     60.225807     51.620968   \n",
       "19     27.477419     27.750969     69.040321     66.491669     65.161293   \n",
       "20     27.468388     28.192667     85.608330     78.500000     69.449997   \n",
       "21     27.693548     27.863333     79.158333     69.056450     68.250000   \n",
       "22  27006.000000  27918.000000     68.000000     63.056454     64.958336   \n",
       "23     26.760645  26764.000000     66.433334     65.854836     75.346771   \n",
       "24     27.911428     26.878065     63.241936     66.150002     65.750000   \n",
       "25     27.992258     28.305332     69.766670     67.991936     63.725807   \n",
       "26     28.294193     28.184286  47075.000000     46.290321  49425.000000   \n",
       "27     24.355333     24.703871     85.922417     84.024193     77.083336   \n",
       "28     26.790001     26.566452     56.629032     61.475807     71.035713   \n",
       "29     26.350668     26.858709     59.357143     71.137100     67.800003   \n",
       "30     27.649033     27.497240  49375.000000     50.879032     51.700001   \n",
       "31     28.819332     27.288387     54.822582     51.185486     51.458332   \n",
       "32     24.878710     24.330000     56.451614     58.603447     68.604836   \n",
       "33     26.917419     25.990000     52.391666     67.032257     72.943550   \n",
       "34  27918.000000     28.101934     63.056454     64.958336     54.379032   \n",
       "35     26.738571     25.836775     48.846775  53175.000000     54.637096   \n",
       "36     28.538710     27.911428     65.341667     63.241936     66.150002   \n",
       "37     26.139999     25.232258     54.153225     55.629032     62.464287   \n",
       "38     25.494194  25484.000000  57225.000000     73.120972     78.854836   \n",
       "39     26.858709     27.625160     71.137100     67.800003     75.750000   \n",
       "40     27.020000     26.101290     50.846775  64275.000000     64.282257   \n",
       "41     26.438709     28.159332     65.966667     66.056450  63775.000000   \n",
       "42     28.549032     26.810324     51.209679     51.612068     55.443546   \n",
       "43     28.410969     28.889334     68.250000  72275.000000     61.911289   \n",
       "44     27.750969  27728.000000     66.491669     65.161293     69.766670   \n",
       "45     27.322580  27916.000000     70.266670     67.919357     71.822578   \n",
       "46     26.746452     27.701935     77.758064     78.391670     77.717743   \n",
       "47     24.703871     26.578711     84.024193     77.083336     73.596771   \n",
       "48     28.040646  27542.000000     71.822578     73.870689     69.040321   \n",
       "49     25.232258     24.765333     55.629032     62.464287     66.145164   \n",
       "50     25.906452     25.684286     51.458332     62.854839  57225.000000   \n",
       "51     25.990000     25.692259     67.032257     72.943550     74.491379   \n",
       "52     26.687332     27.129032     76.642860     83.072578     85.283333   \n",
       "53     24.670000     24.801291     69.223213     75.500000     74.916664   \n",
       "54     28.205807     28.108387     62.991936     62.408333     58.935486   \n",
       "55     28.949333     28.549032     52.056454     51.209679     51.612068   \n",
       "56     28.494194     28.949333     72.608330     52.056454     51.209679   \n",
       "\n",
       "         feat_22       feat_23       feat_24  PRECIPITACAO TOTAL MENSAL(mm)  \\\n",
       "0      62.408333     58.935486     49.691666                       3.000000   \n",
       "1      56.451614     58.603447     68.604836                     139.600006   \n",
       "2      69.056450     68.250000  72275.000000                       0.000000   \n",
       "3      60.080357     62.709679     74.508331                      93.400002   \n",
       "4      68.250000  72275.000000     61.911289                       0.000000   \n",
       "5      51.458332     62.854839  57225.000000                      86.900002   \n",
       "6      50.879032     51.700001     55.290321                      83.599998   \n",
       "7      69.189651     61.290321     65.341667                      23.600000   \n",
       "8      63.653225     70.298386     80.767860                      97.500000   \n",
       "9   64275.000000     64.282257     63.225807                     175.800003   \n",
       "10     58.908333     56.629032     61.475807                     122.900002   \n",
       "11     62.258930     67.532257  71025.000000                     166.699997   \n",
       "12     69.449997     66.379028     57.233871                       0.000000   \n",
       "13     58.603447     68.604836     65.966667                     132.399994   \n",
       "14     63.725807     64.949997     65.040321                       0.000000   \n",
       "15     55.629032     62.464287     66.145164                     135.399994   \n",
       "16     65.080643     66.433334     65.854836                       6.200000   \n",
       "17     67.532257  71025.000000     78.750000                      58.200001   \n",
       "18     50.058334     50.846775  64275.000000                      11.300000   \n",
       "19     69.766670     67.991936     63.725807                       0.000000   \n",
       "20     66.379028     57.233871     59.008331                       0.000000   \n",
       "21  72275.000000     61.911289     65.683334                       0.000000   \n",
       "22     54.379032     51.049999  49375.000000                       0.000000   \n",
       "23     76.642860     83.072578     85.283333                     130.899994   \n",
       "24     66.209679     72.053574     78.266129                      50.500000   \n",
       "25     64.949997     65.040321     60.541668                       0.600000   \n",
       "26     49.814514     57.677418     59.357143                     169.600006   \n",
       "27     73.596771  82625.000000     76.298386                      27.799999   \n",
       "28     77.758064     78.391670     77.717743                      82.500000   \n",
       "29     75.750000     58.791668     49.911289                       0.000000   \n",
       "30     55.290321     56.451614     58.603447                     250.399994   \n",
       "31     62.854839  57225.000000     73.120972                     137.699997   \n",
       "32     65.966667     66.056450  63775.000000                      49.000000   \n",
       "33     74.491379     77.806450     77.983330                      91.000000   \n",
       "34     51.049999  49375.000000     50.879032                       0.000000   \n",
       "35     59.556454     69.223213     75.500000                     320.700012   \n",
       "36     65.750000     66.209679     72.053574                     160.699997   \n",
       "37     66.145164     70.583336     70.258064                      40.000000   \n",
       "38     85.482140     81.838707  82925.000000                     120.699997   \n",
       "39     58.791668     49.911289     46.298386                       1.700000   \n",
       "40     63.225807     74.357140     74.169357                      96.500000   \n",
       "41     59.612904     51.879032     46.549999                       0.000000   \n",
       "42     58.666668     72.362900     84.072578                     197.600006   \n",
       "43     65.683334     65.927422  64575.000000                       6.200000   \n",
       "44     67.991936     63.725807     64.949997                       0.300000   \n",
       "45     73.870689     69.040321     66.491669                      21.000000   \n",
       "46     72.608330     52.056454     51.209679                       0.000000   \n",
       "47  82625.000000     76.298386     63.774193                       0.000000   \n",
       "48     66.491669     65.161293     69.766670                       0.500000   \n",
       "49     70.583336     70.258064     66.358330                       9.700000   \n",
       "50     73.120972     78.854836     85.482140                     156.600006   \n",
       "51     77.806450     77.983330     78.540321                      95.099998   \n",
       "52     85.056450     80.891670     69.189651                       0.000000   \n",
       "53     77.612900     76.641670     73.620972                      54.099998   \n",
       "54     49.691666     49.967743     50.209679                       0.000000   \n",
       "55     55.443546     58.666668     72.362900                     305.399994   \n",
       "56     51.612068     55.443546     58.666668                      27.400000   \n",
       "\n",
       "         Label  \n",
       "0    40.330205  \n",
       "1   114.257491  \n",
       "2    52.508798  \n",
       "3    87.522940  \n",
       "4     6.620884  \n",
       "5    33.103750  \n",
       "6    70.396729  \n",
       "7    29.332732  \n",
       "8   102.326448  \n",
       "9    56.455567  \n",
       "10   66.840665  \n",
       "11   67.049286  \n",
       "12   21.145885  \n",
       "13  102.403397  \n",
       "14   25.023252  \n",
       "15   87.322210  \n",
       "16   48.675951  \n",
       "17   66.870437  \n",
       "18   31.440966  \n",
       "19   20.397390  \n",
       "20   23.099659  \n",
       "21   29.703259  \n",
       "22   11.685841  \n",
       "23   77.637458  \n",
       "24   97.217337  \n",
       "25   32.934660  \n",
       "26   91.687071  \n",
       "27   35.004238  \n",
       "28   70.676454  \n",
       "29   34.473692  \n",
       "30   83.409500  \n",
       "31   88.361714  \n",
       "32   60.994997  \n",
       "33   67.862797  \n",
       "34   18.530082  \n",
       "35  111.691988  \n",
       "36   86.164270  \n",
       "37   78.183198  \n",
       "38   91.092026  \n",
       "39   10.159690  \n",
       "40   86.139753  \n",
       "41    8.901714  \n",
       "42  114.341114  \n",
       "43   17.246861  \n",
       "44   18.966188  \n",
       "45   72.803636  \n",
       "46   21.280452  \n",
       "47   24.415277  \n",
       "48   50.311065  \n",
       "49   75.827174  \n",
       "50  103.010815  \n",
       "51   47.868647  \n",
       "52   18.047291  \n",
       "53   42.577139  \n",
       "54   27.461373  \n",
       "55   47.306728  \n",
       "56   53.355519  \n",
       "\n",
       "[57 rows x 26 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_model(model_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b38c7a5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "This function only supports tree based models for binary classification: xgboost, rf, et, catboost, lightgbm, dt.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3263086/459304727.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minterpret_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_tuned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/pycaret/regression.py\u001b[0m in \u001b[0;36minterpret_model\u001b[0;34m(estimator, plot, feature, observation, use_train_data, X_new_sample, y_new_sample, save, **kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m     \"\"\"\n\u001b[1;32m   1664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m     return pycaret.internal.tabular.interpret_model(\n\u001b[0m\u001b[1;32m   1666\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages/pycaret/internal/tabular.py\u001b[0m in \u001b[0;36minterpret_model\u001b[0;34m(estimator, plot, feature, observation, use_train_data, X_new_sample, y_new_sample, save, **kwargs)\u001b[0m\n\u001b[1;32m   7532\u001b[0m         \u001b[0mmodel_id\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshap_models_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7533\u001b[0m     ):\n\u001b[0;32m-> 7534\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m   7535\u001b[0m             \u001b[0;34mf\"This function only supports tree based models for binary classification: {', '.join(shap_models_ids)}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7536\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: This function only supports tree based models for binary classification: xgboost, rf, et, catboost, lightgbm, dt."
     ]
    }
   ],
   "source": [
    "interpret_model(model_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6b33fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\n",
      "  Downloading shap-0.40.0-cp38-cp38-manylinux2010_x86_64.whl (571 kB)\n",
      "\u001b[K     |████████████████████████████████| 571 kB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages (from shap) (0.23.2)\n",
      "Requirement already satisfied: packaging>20.9 in /home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages (from shap) (21.0)\n",
      "Requirement already satisfied: scipy in /home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages (from shap) (1.5.4)\n",
      "Requirement already satisfied: numpy in /home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages (from shap) (1.19.5)\n",
      "Requirement already satisfied: numba in /home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages (from shap) (0.54.0)\n",
      "Requirement already satisfied: pandas in /home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages (from shap) (1.3.2)\n",
      "Requirement already satisfied: cloudpickle in /home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages (from shap) (1.6.0)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages (from shap) (4.62.2)\n",
      "Collecting slicer==0.0.7\n",
      "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages (from packaging>20.9->shap) (2.4.7)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages (from numba->shap) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages (from numba->shap) (51.3.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages (from pandas->shap) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->shap) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages (from scikit-learn->shap) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/9679247/Documentos/VirtualEnvs/PyCaret/lib/python3.8/site-packages (from scikit-learn->shap) (2.2.0)\n",
      "Installing collected packages: slicer, shap\n",
      "Successfully installed shap-0.40.0 slicer-0.0.7\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/9679247/Documentos/VirtualEnvs/PyCaret/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93891e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
